{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ThesisPackage.Environments.multi_pong_sender_receiver_ball_onehot import PongEnvSenderReceiverBallOneHot\n",
    "from ThesisPackage.RL.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from ThesisPackage.RL.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "from ThesisPackage.Wrappers.vecWrapper import PettingZooVectorizationParallelWrapper\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(max_episode_steps = 1024, sequence_length = 1, vocab_size = 3):\n",
    "    env = PongEnvSenderReceiverBallOneHot(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_episode_steps=max_episode_steps)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", sequence_length=1, vocab_size=3):\n",
    "    env = make_env(sequence_length=sequence_length, vocab_size=vocab_size)\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pong\" in model:\n",
    "            state_dict = torch.load(os.path.join(path, model))\n",
    "            timestamp = model.split(\"_\")[-1]\n",
    "            timestamp = int(timestamp.split(\".\")[0])\n",
    "            agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "            agent.agent.load_state_dict(state_dict)\n",
    "            models[timestamp] = agent\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def record_data(env, data):\n",
    "    data[\"ball_1\"].append(copy.deepcopy(env.balls[\"ball_1\"][\"position\"]))\n",
    "    data[\"ball_2\"].append(copy.deepcopy(env.balls[\"ball_2\"][\"position\"]))\n",
    "    data[\"direction_1\"].append(copy.deepcopy(env.balls[\"ball_1\"][\"direction\"]))\n",
    "    data[\"direction_2\"].append(copy.deepcopy(env.balls[\"ball_2\"][\"direction\"]))\n",
    "    data[\"paddle_1\"].append(copy.deepcopy(env.paddles[\"paddle_1\"]))\n",
    "    data[\"paddle_2\"].append(copy.deepcopy(env.paddles[\"paddle_2\"]))\n",
    "    data[\"distances\"].append(copy.deepcopy(abs(env.paddles[\"paddle_1\"] - env.paddles[\"paddle_2\"])))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation(env, agent, epochs=1, tracking_agent=\"paddle_1\", threshold=0.002, use_perturbation=False, above_threshold=False):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "    data = {\"ball_1\": [], \"ball_2\": [], \"direction_1\":[], \"direction_2\":[], \"paddle_1\": [], \"paddle_2\": [], \"distances\":[]}\n",
    "\n",
    "    for i in range(epochs):\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            timestep += 1\n",
    "            tokens.append(obs[tracking_agent][-1 * env.sequence_length:])\n",
    "            obs = [obs]\n",
    "            state = [state]\n",
    "            obs = np.array(flatten_list(obs))\n",
    "            state = np.array(flatten_list(state))\n",
    "\n",
    "            data = record_data(env, data)\n",
    "            \n",
    "            # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "            language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "            language_importances.append(language_perturbation)\n",
    "\n",
    "            # If any of language_importances is higher thnan 0.002\n",
    "            if any([importance >= threshold for importance in language_perturbation]) and use_perturbation and above_threshold:\n",
    "                language_observations = obs[:, -1 * env.vocab_size * env.sequence_length:]\n",
    "                random_language = np.random.randint(0, env.vocab_size, (language_observations.shape[0], env.sequence_length))\n",
    "                random_language = np.eye(env.vocab_size)[random_language]\n",
    "                random_language = random_language.reshape(language_observations.shape[0], env.sequence_length * env.vocab_size)\n",
    "                obs[:, -1 * env.vocab_size * env.sequence_length:] = random_language\n",
    "\n",
    "            if all([importance <= threshold for importance in language_perturbation]) and use_perturbation and not above_threshold:\n",
    "                language_observations = obs[:, -1 * env.vocab_size * env.sequence_length:]\n",
    "                random_language = np.random.randint(0, env.vocab_size, (language_observations.shape[0], env.sequence_length))\n",
    "                random_language = np.eye(env.vocab_size)[random_language]\n",
    "                random_language = random_language.reshape(language_observations.shape[0], env.sequence_length * env.vocab_size)\n",
    "                obs[:, -1 * env.vocab_size * env.sequence_length:] = random_language\n",
    "\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "                actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "            actions = actions[0]\n",
    "            actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "            obs, _, truncations, terminations, infos = env.step(actions)\n",
    "            state = env.state()\n",
    "\n",
    "            if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "                average_length.append(timestep)\n",
    "                obs, info = env.reset()\n",
    "                state = env.state()\n",
    "                break\n",
    "    return np.array(language_importances), average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x38584d600>, 2: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x33e600700>, 3: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x38584d990>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directories = os.listdir(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/\")\n",
    "directories = [model for model in directories if \".DS_Store\" not in model]\n",
    "models = {}\n",
    "for directory in directories:\n",
    "    sequence_length = int(directory.split(\"_\")[-1])\n",
    "    agents = load(f\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/{directory}\", sequence_length=sequence_length)\n",
    "    agent_indizes = list(agents.keys())\n",
    "    agent_indizes.sort()\n",
    "    models[sequence_length] = agents[agent_indizes[-1]]\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share_above_threshold(arr, threshold=0.15):\n",
    "    # Check if each element is greater than the threshold\n",
    "    condition = arr > threshold\n",
    "    # Count rows where at least one element satisfies the condition\n",
    "    count = np.any(condition, axis=1).sum()\n",
    "    # Calculate the share\n",
    "    share = count / arr.shape[0]\n",
    "    return share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "threshold = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sequence length 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sequence length 2\n",
      "Testing sequence length 3\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "no_noise_resutls = {}\n",
    "for sequence_length, model in models.items():\n",
    "    print(f\"Testing sequence length {sequence_length}\")\n",
    "    results[sequence_length] = {}\n",
    "    env = make_env(sequence_length=sequence_length)\n",
    "    no_noise_importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=False)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"no noise\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": noise_share}\n",
    "    no_noise_resutls[sequence_length] = no_noise_importances\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=True, above_threshold=True)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"above threshold\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": noise_share}\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=True, above_threshold=False)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"below threshold\"] = {\"lengths\": np.mean(lengths), \"share_below_threshold\": 1 - noise_share}\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=0.00, use_perturbation=True, above_threshold=True)\n",
    "    results[sequence_length][\"all noise\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'no noise': {'lengths': 843.99, 'share_above_threshold': 0.49174753255370324}, 'above threshold': {'lengths': 443.88, 'share_above_threshold': 0.4770658736595476}, 'below threshold': {'lengths': 803.13, 'share_below_threshold': 0.5113244431163074}, 'all noise': {'lengths': 460.4, 'share_above_threshold': 1}}, 2: {'no noise': {'lengths': 814.92, 'share_above_threshold': 0.2752171992342807}, 'above threshold': {'lengths': 438.84, 'share_above_threshold': 0.258249020144016}, 'below threshold': {'lengths': 809.63, 'share_below_threshold': 0.7205266603263218}, 'all noise': {'lengths': 427.4, 'share_above_threshold': 1}}, 3: {'no noise': {'lengths': 876.28, 'share_above_threshold': 0.2770689733875017}, 'above threshold': {'lengths': 379.26, 'share_above_threshold': 0.2593735168485999}, 'below threshold': {'lengths': 845.86, 'share_below_threshold': 0.7317995885843993}, 'all noise': {'lengths': 436.68, 'share_above_threshold': 1}}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_importances(data, title):\n",
    "    # Plot the two lines\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(data[:, 0], label='Agent 1')\n",
    "    plt.plot(data[:, 1], label='Agent 2')\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([0, len(data)])\n",
    "    ax.set_ylabel('KL Divergence')\n",
    "    ax.set_xlabel('Timestep')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/Perturbation/screenshot_perturbs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importances(no_noise_resutls[1][0], \"No Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h!]\n",
      "\\centering\n",
      "\\begin{tabular}{|c|c|c|c|c|}\n",
      "\\hline\n",
      "\\textbf{Sequence Length} & \\textbf{No Noise} & \\textbf{\\textless T=0.02} & \\textbf{\\textgreater T=0.02} & \\textbf{All Noise} \\\\\n",
      "\\hline\n",
      "1 & 844.0 & 803.1 & 443.9 & 460.4 \\\\\n",
      "\\hline\n",
      "2 & 814.9 & 809.6 & 438.8 & 427.4 \\\\\n",
      "\\hline\n",
      "3 & 876.3 & 845.9 & 379.3 & 436.7 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{MA Pong Noise Test}\n",
      "\\label{table:ma_pong}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table(data):\n",
    "    latex_table = \"\\\\begin{table}[h!]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|c|c|c|}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Sequence Length} & \\\\textbf{No Noise} & \\\\textbf{\\\\textless T=0.02} & \\\\textbf{\\\\textgreater T=0.02} & \\\\textbf{All Noise} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    for seq_len, values in data.items():\n",
    "        no_noise_len = values['no noise']['lengths']\n",
    "        above_threshold_len = values['below threshold']['lengths']\n",
    "        noised_share_len = values['above threshold']['lengths']\n",
    "        all_noise_len = values['all noise']['lengths']\n",
    "        latex_table += f\"{seq_len} & {no_noise_len:.1f} & {above_threshold_len:.1f} & {noised_share_len:.1f} & {all_noise_len:.1f} \\\\\\\\\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    latex_table += \"\\\\end{tabular}\\n\\\\caption{MA Pong Noise Test}\\n\\\\label{table:ma_pong}\\n\\\\end{table}\\n\"\n",
    "    return latex_table\n",
    "\n",
    "# Generate and print the LaTeX table\n",
    "latex_table = generate_latex_table(results)\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
