{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ThesisPackage.Environments.multi_pong_sender_receiver_ball_onehot import PongEnvSenderReceiverBallOneHot\n",
    "from ThesisPackage.RL.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from ThesisPackage.RL.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "from ThesisPackage.Wrappers.vecWrapper import PettingZooVectorizationParallelWrapper\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(max_episode_steps = 1024, sequence_length = 1, vocab_size = 3):\n",
    "    env = PongEnvSenderReceiverBallOneHot(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_episode_steps=max_episode_steps)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", sequence_length=1, vocab_size=3):\n",
    "    env = make_env(sequence_length=sequence_length, vocab_size=vocab_size)\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pong\" in model:\n",
    "            state_dict = torch.load(os.path.join(path, model))\n",
    "            timestamp = model.split(\"_\")[-1]\n",
    "            timestamp = int(timestamp.split(\".\")[0])\n",
    "            agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "            agent.agent.load_state_dict(state_dict)\n",
    "            models[timestamp] = agent\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation(env, agent, threshold=0.02, number_samples=30000):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "    data = {\"paddle_1\": [], \"paddle_2\": [], \"paddle_1_obs\": [], \"paddle_2_obs\": [], \"ball_1_pos\": [], \"ball_2_pos\": []}\n",
    "\n",
    "    timestep = 0\n",
    "    while True:\n",
    "        for cur_agent in env.agents:\n",
    "            data[cur_agent].append(copy.deepcopy(env.paddles[cur_agent]))\n",
    "            data[cur_agent + \"_obs\"].append(copy.deepcopy(obs[cur_agent]))\n",
    "        for cur_ball in env.balls.keys():\n",
    "            data[cur_ball + \"_pos\"].append(copy.deepcopy(env.balls[cur_ball][\"position\"]))\n",
    "        obs = [obs]\n",
    "        state = [state]\n",
    "        obs = np.array(flatten_list(obs))\n",
    "        state = np.array(flatten_list(state))\n",
    "        \n",
    "        # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "        language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "        language_importances.append(language_perturbation)\n",
    "\n",
    "        if np.any(language_perturbation > threshold):\n",
    "            timestep += 1\n",
    "\n",
    "        if timestep > number_samples:\n",
    "            break\n",
    "\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "            actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "        actions = actions[0]\n",
    "        actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "        obs, _, truncations, terminations, infos = env.step(actions)\n",
    "        state = env.state()\n",
    "\n",
    "        if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "            average_length.append(env.timestep)\n",
    "            obs, info = env.reset()\n",
    "            state = env.state()\n",
    "    return np.array(language_importances), data, average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 2: Define the dataset and dataloader\n",
    "class PositionDataset(Dataset):\n",
    "    def __init__(self, data, labels, device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Step 3: Define the model architecture\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, dataloader, input_size, learning_rate, device):\n",
    "    \n",
    "    model = SimpleClassifier(input_size).to(device)\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        # scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return epoch_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chance_level(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "\n",
    "    chance_level = counts[index] / len(labels)\n",
    "    return chance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_training_data(language_importances_indices, data, sequence_length=1, vocab_size=3):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices][3:-3]\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    labels = np.zeros(len(paddle_1_indices))\n",
    "    labels = np.where(np.array(data[\"paddle_1\"])[paddle_1_indices] > np.array(data[\"paddle_2\"])[paddle_1_indices], 1, 0)\n",
    "\n",
    "    paddle_1_obs = np.array(data[\"paddle_1_obs\"])\n",
    "    paddle_1_obs = np.array([paddle_1_obs[index - 3:index + 3] for index in paddle_1_indices])\n",
    "    player_1_lang = paddle_1_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_1_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_1_lang = player_1_lang.reshape(new_shape)\n",
    "\n",
    "    paddle_2_obs = np.array(data[\"paddle_2_obs\"])\n",
    "    paddle_2_obs = np.array([paddle_2_obs[index - 3:index + 3] for index in paddle_1_indices])\n",
    "    player_2_lang = paddle_2_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_2_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_2_lang = player_2_lang.reshape(new_shape)\n",
    "\n",
    "    inputs = np.concatenate((player_1_lang, player_2_lang), axis=1)\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def generate_training_data_obs(language_importances_indices, data, sequence_length=1, vocab_size=3):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "\n",
    "    paddle_1_data = np.array(data[\"paddle_1_obs\"])[paddle_1_indices]\n",
    "    labels = np.zeros_like(paddle_1_indices)\n",
    "    labels = np.where(np.array(data[\"paddle_1\"])[paddle_1_indices] > np.array(data[\"paddle_2\"])[paddle_1_indices], 1, 0)\n",
    "\n",
    "    paddle_1_data_no_lang = paddle_1_data[:, :-1 * sequence_length * vocab_size]\n",
    "\n",
    "    return paddle_1_data, labels, paddle_1_data_no_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directories = os.listdir(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/\")\n",
    "directories = [model for model in directories if \".DS_Store\" not in model]\n",
    "models = {}\n",
    "for directory in directories:\n",
    "    sequence_length = int(directory.split(\"_\")[-1])\n",
    "    agents = load(f\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/{directory}\", sequence_length=sequence_length)\n",
    "    agent_indizes = list(agents.keys())\n",
    "    agent_indizes.sort()\n",
    "    models[sequence_length] = agents[agent_indizes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Loss: 0.5356, Accuracy: 0.6868\n",
      "Epoch 2/120, Loss: 0.5299, Accuracy: 0.6892\n",
      "Epoch 3/120, Loss: 0.5284, Accuracy: 0.6901\n",
      "Epoch 4/120, Loss: 0.5278, Accuracy: 0.6911\n",
      "Epoch 5/120, Loss: 0.5273, Accuracy: 0.6912\n",
      "Epoch 6/120, Loss: 0.5269, Accuracy: 0.6903\n",
      "Epoch 7/120, Loss: 0.5267, Accuracy: 0.6914\n",
      "Epoch 8/120, Loss: 0.5263, Accuracy: 0.6910\n",
      "Epoch 9/120, Loss: 0.5263, Accuracy: 0.6915\n",
      "Epoch 10/120, Loss: 0.5262, Accuracy: 0.6905\n",
      "Epoch 11/120, Loss: 0.5259, Accuracy: 0.6904\n",
      "Epoch 12/120, Loss: 0.5260, Accuracy: 0.6908\n",
      "Epoch 13/120, Loss: 0.5259, Accuracy: 0.6907\n",
      "Epoch 14/120, Loss: 0.5258, Accuracy: 0.6906\n",
      "Epoch 15/120, Loss: 0.5258, Accuracy: 0.6912\n",
      "Epoch 16/120, Loss: 0.5258, Accuracy: 0.6914\n",
      "Epoch 17/120, Loss: 0.5256, Accuracy: 0.6908\n",
      "Epoch 18/120, Loss: 0.5258, Accuracy: 0.6912\n",
      "Epoch 19/120, Loss: 0.5255, Accuracy: 0.6911\n",
      "Epoch 20/120, Loss: 0.5257, Accuracy: 0.6909\n",
      "Epoch 21/120, Loss: 0.5255, Accuracy: 0.6903\n",
      "Epoch 22/120, Loss: 0.5255, Accuracy: 0.6914\n",
      "Epoch 23/120, Loss: 0.5257, Accuracy: 0.6909\n",
      "Epoch 24/120, Loss: 0.5257, Accuracy: 0.6908\n",
      "Epoch 25/120, Loss: 0.5257, Accuracy: 0.6916\n",
      "Epoch 26/120, Loss: 0.5257, Accuracy: 0.6911\n",
      "Epoch 27/120, Loss: 0.5258, Accuracy: 0.6904\n",
      "Epoch 28/120, Loss: 0.5257, Accuracy: 0.6899\n",
      "Epoch 29/120, Loss: 0.5258, Accuracy: 0.6904\n",
      "Epoch 30/120, Loss: 0.5257, Accuracy: 0.6907\n",
      "Epoch 31/120, Loss: 0.5257, Accuracy: 0.6905\n",
      "Epoch 32/120, Loss: 0.5253, Accuracy: 0.6914\n",
      "Epoch 33/120, Loss: 0.5254, Accuracy: 0.6910\n",
      "Epoch 34/120, Loss: 0.5255, Accuracy: 0.6906\n",
      "Epoch 35/120, Loss: 0.5254, Accuracy: 0.6905\n",
      "Epoch 36/120, Loss: 0.5258, Accuracy: 0.6912\n",
      "Epoch 37/120, Loss: 0.5255, Accuracy: 0.6908\n",
      "Epoch 38/120, Loss: 0.5253, Accuracy: 0.6907\n",
      "Epoch 39/120, Loss: 0.5255, Accuracy: 0.6914\n",
      "Epoch 40/120, Loss: 0.5253, Accuracy: 0.6918\n",
      "Epoch 41/120, Loss: 0.5253, Accuracy: 0.6910\n",
      "Epoch 42/120, Loss: 0.5253, Accuracy: 0.6920\n",
      "Epoch 43/120, Loss: 0.5254, Accuracy: 0.6910\n",
      "Epoch 44/120, Loss: 0.5253, Accuracy: 0.6916\n",
      "Epoch 45/120, Loss: 0.5253, Accuracy: 0.6909\n",
      "Epoch 46/120, Loss: 0.5254, Accuracy: 0.6902\n",
      "Epoch 47/120, Loss: 0.5252, Accuracy: 0.6915\n",
      "Epoch 48/120, Loss: 0.5254, Accuracy: 0.6906\n",
      "Epoch 49/120, Loss: 0.5253, Accuracy: 0.6918\n",
      "Epoch 50/120, Loss: 0.5254, Accuracy: 0.6913\n",
      "Epoch 51/120, Loss: 0.5255, Accuracy: 0.6911\n",
      "Epoch 52/120, Loss: 0.5253, Accuracy: 0.6913\n",
      "Epoch 53/120, Loss: 0.5255, Accuracy: 0.6910\n",
      "Epoch 54/120, Loss: 0.5254, Accuracy: 0.6910\n",
      "Epoch 55/120, Loss: 0.5254, Accuracy: 0.6915\n",
      "Epoch 56/120, Loss: 0.5255, Accuracy: 0.6907\n",
      "Epoch 57/120, Loss: 0.5255, Accuracy: 0.6917\n",
      "Epoch 58/120, Loss: 0.5256, Accuracy: 0.6911\n",
      "Epoch 59/120, Loss: 0.5255, Accuracy: 0.6907\n",
      "Epoch 60/120, Loss: 0.5255, Accuracy: 0.6908\n",
      "Epoch 61/120, Loss: 0.5257, Accuracy: 0.6912\n",
      "Epoch 62/120, Loss: 0.5256, Accuracy: 0.6906\n",
      "Epoch 63/120, Loss: 0.5255, Accuracy: 0.6906\n",
      "Epoch 64/120, Loss: 0.5257, Accuracy: 0.6909\n",
      "Epoch 65/120, Loss: 0.5259, Accuracy: 0.6906\n",
      "Epoch 66/120, Loss: 0.5259, Accuracy: 0.6906\n",
      "Epoch 67/120, Loss: 0.5262, Accuracy: 0.6909\n",
      "Epoch 68/120, Loss: 0.5259, Accuracy: 0.6913\n",
      "Epoch 69/120, Loss: 0.5258, Accuracy: 0.6900\n",
      "Epoch 70/120, Loss: 0.5257, Accuracy: 0.6907\n",
      "Epoch 71/120, Loss: 0.5256, Accuracy: 0.6911\n",
      "Epoch 72/120, Loss: 0.5258, Accuracy: 0.6916\n",
      "Epoch 73/120, Loss: 0.5258, Accuracy: 0.6905\n",
      "Epoch 74/120, Loss: 0.5256, Accuracy: 0.6915\n",
      "Epoch 75/120, Loss: 0.5257, Accuracy: 0.6903\n",
      "Epoch 76/120, Loss: 0.5257, Accuracy: 0.6906\n",
      "Epoch 77/120, Loss: 0.5259, Accuracy: 0.6910\n",
      "Epoch 78/120, Loss: 0.5257, Accuracy: 0.6908\n",
      "Epoch 79/120, Loss: 0.5260, Accuracy: 0.6896\n",
      "Epoch 80/120, Loss: 0.5260, Accuracy: 0.6898\n",
      "Epoch 81/120, Loss: 0.5260, Accuracy: 0.6898\n",
      "Epoch 82/120, Loss: 0.5259, Accuracy: 0.6904\n",
      "Epoch 83/120, Loss: 0.5259, Accuracy: 0.6903\n",
      "Epoch 84/120, Loss: 0.5259, Accuracy: 0.6908\n",
      "Epoch 85/120, Loss: 0.5261, Accuracy: 0.6908\n",
      "Epoch 86/120, Loss: 0.5259, Accuracy: 0.6912\n",
      "Epoch 87/120, Loss: 0.5260, Accuracy: 0.6908\n",
      "Epoch 88/120, Loss: 0.5258, Accuracy: 0.6897\n",
      "Epoch 89/120, Loss: 0.5262, Accuracy: 0.6899\n",
      "Epoch 90/120, Loss: 0.5262, Accuracy: 0.6898\n",
      "Epoch 91/120, Loss: 0.5261, Accuracy: 0.6898\n",
      "Epoch 92/120, Loss: 0.5260, Accuracy: 0.6902\n",
      "Epoch 93/120, Loss: 0.5260, Accuracy: 0.6912\n",
      "Epoch 94/120, Loss: 0.5259, Accuracy: 0.6906\n",
      "Epoch 95/120, Loss: 0.5260, Accuracy: 0.6903\n",
      "Epoch 96/120, Loss: 0.5262, Accuracy: 0.6899\n",
      "Epoch 97/120, Loss: 0.5259, Accuracy: 0.6904\n",
      "Epoch 98/120, Loss: 0.5263, Accuracy: 0.6902\n",
      "Epoch 99/120, Loss: 0.5261, Accuracy: 0.6900\n",
      "Epoch 100/120, Loss: 0.5265, Accuracy: 0.6887\n",
      "Epoch 101/120, Loss: 0.5264, Accuracy: 0.6889\n",
      "Epoch 102/120, Loss: 0.5269, Accuracy: 0.6884\n",
      "Epoch 103/120, Loss: 0.5268, Accuracy: 0.6897\n",
      "Epoch 104/120, Loss: 0.5264, Accuracy: 0.6891\n",
      "Epoch 105/120, Loss: 0.5265, Accuracy: 0.6897\n",
      "Epoch 106/120, Loss: 0.5270, Accuracy: 0.6889\n",
      "Epoch 107/120, Loss: 0.5265, Accuracy: 0.6903\n",
      "Epoch 108/120, Loss: 0.5266, Accuracy: 0.6904\n",
      "Epoch 109/120, Loss: 0.5265, Accuracy: 0.6899\n",
      "Epoch 110/120, Loss: 0.5266, Accuracy: 0.6893\n",
      "Epoch 111/120, Loss: 0.5267, Accuracy: 0.6901\n",
      "Epoch 112/120, Loss: 0.5268, Accuracy: 0.6902\n",
      "Epoch 113/120, Loss: 0.5264, Accuracy: 0.6901\n",
      "Epoch 114/120, Loss: 0.5266, Accuracy: 0.6898\n",
      "Epoch 115/120, Loss: 0.5263, Accuracy: 0.6906\n",
      "Epoch 116/120, Loss: 0.5267, Accuracy: 0.6892\n",
      "Epoch 117/120, Loss: 0.5266, Accuracy: 0.6902\n",
      "Epoch 118/120, Loss: 0.5268, Accuracy: 0.6900\n",
      "Epoch 119/120, Loss: 0.5267, Accuracy: 0.6902\n",
      "Epoch 120/120, Loss: 0.5271, Accuracy: 0.6891\n",
      "Training complete!\n",
      "Accuracy: 0.6883\n",
      "Epoch 1/120, Loss: 0.5359, Accuracy: 0.6870\n",
      "Epoch 2/120, Loss: 0.5293, Accuracy: 0.6894\n",
      "Epoch 3/120, Loss: 0.5280, Accuracy: 0.6900\n",
      "Epoch 4/120, Loss: 0.5270, Accuracy: 0.6909\n",
      "Epoch 5/120, Loss: 0.5263, Accuracy: 0.6912\n",
      "Epoch 6/120, Loss: 0.5258, Accuracy: 0.6921\n",
      "Epoch 7/120, Loss: 0.5254, Accuracy: 0.6923\n",
      "Epoch 8/120, Loss: 0.5250, Accuracy: 0.6923\n",
      "Epoch 9/120, Loss: 0.5246, Accuracy: 0.6919\n",
      "Epoch 10/120, Loss: 0.5245, Accuracy: 0.6931\n",
      "Epoch 11/120, Loss: 0.5241, Accuracy: 0.6927\n",
      "Epoch 12/120, Loss: 0.5240, Accuracy: 0.6935\n",
      "Epoch 13/120, Loss: 0.5236, Accuracy: 0.6936\n",
      "Epoch 14/120, Loss: 0.5235, Accuracy: 0.6938\n",
      "Epoch 15/120, Loss: 0.5233, Accuracy: 0.6937\n",
      "Epoch 16/120, Loss: 0.5233, Accuracy: 0.6938\n",
      "Epoch 17/120, Loss: 0.5231, Accuracy: 0.6941\n",
      "Epoch 18/120, Loss: 0.5229, Accuracy: 0.6943\n",
      "Epoch 19/120, Loss: 0.5226, Accuracy: 0.6947\n",
      "Epoch 20/120, Loss: 0.5227, Accuracy: 0.6947\n",
      "Epoch 21/120, Loss: 0.5225, Accuracy: 0.6953\n",
      "Epoch 22/120, Loss: 0.5224, Accuracy: 0.6953\n",
      "Epoch 23/120, Loss: 0.5225, Accuracy: 0.6947\n",
      "Epoch 24/120, Loss: 0.5223, Accuracy: 0.6946\n",
      "Epoch 25/120, Loss: 0.5222, Accuracy: 0.6950\n",
      "Epoch 26/120, Loss: 0.5222, Accuracy: 0.6952\n",
      "Epoch 27/120, Loss: 0.5219, Accuracy: 0.6953\n",
      "Epoch 28/120, Loss: 0.5218, Accuracy: 0.6950\n",
      "Epoch 29/120, Loss: 0.5219, Accuracy: 0.6955\n",
      "Epoch 30/120, Loss: 0.5218, Accuracy: 0.6959\n",
      "Epoch 31/120, Loss: 0.5218, Accuracy: 0.6965\n",
      "Epoch 32/120, Loss: 0.5218, Accuracy: 0.6957\n",
      "Epoch 33/120, Loss: 0.5219, Accuracy: 0.6956\n",
      "Epoch 34/120, Loss: 0.5218, Accuracy: 0.6958\n",
      "Epoch 35/120, Loss: 0.5217, Accuracy: 0.6960\n",
      "Epoch 36/120, Loss: 0.5217, Accuracy: 0.6960\n",
      "Epoch 37/120, Loss: 0.5216, Accuracy: 0.6964\n",
      "Epoch 38/120, Loss: 0.5216, Accuracy: 0.6963\n",
      "Epoch 39/120, Loss: 0.5215, Accuracy: 0.6961\n",
      "Epoch 40/120, Loss: 0.5215, Accuracy: 0.6965\n",
      "Epoch 41/120, Loss: 0.5214, Accuracy: 0.6966\n",
      "Epoch 42/120, Loss: 0.5214, Accuracy: 0.6964\n",
      "Epoch 43/120, Loss: 0.5215, Accuracy: 0.6964\n",
      "Epoch 44/120, Loss: 0.5214, Accuracy: 0.6970\n",
      "Epoch 45/120, Loss: 0.5213, Accuracy: 0.6971\n",
      "Epoch 46/120, Loss: 0.5213, Accuracy: 0.6969\n",
      "Epoch 47/120, Loss: 0.5213, Accuracy: 0.6970\n",
      "Epoch 48/120, Loss: 0.5212, Accuracy: 0.6970\n",
      "Epoch 49/120, Loss: 0.5212, Accuracy: 0.6967\n",
      "Epoch 50/120, Loss: 0.5211, Accuracy: 0.6970\n",
      "Epoch 51/120, Loss: 0.5213, Accuracy: 0.6972\n",
      "Epoch 52/120, Loss: 0.5212, Accuracy: 0.6968\n",
      "Epoch 53/120, Loss: 0.5212, Accuracy: 0.6968\n",
      "Epoch 54/120, Loss: 0.5213, Accuracy: 0.6967\n",
      "Epoch 55/120, Loss: 0.5212, Accuracy: 0.6965\n",
      "Epoch 56/120, Loss: 0.5213, Accuracy: 0.6967\n",
      "Epoch 57/120, Loss: 0.5211, Accuracy: 0.6962\n",
      "Epoch 58/120, Loss: 0.5211, Accuracy: 0.6970\n",
      "Epoch 59/120, Loss: 0.5212, Accuracy: 0.6966\n",
      "Epoch 60/120, Loss: 0.5212, Accuracy: 0.6972\n",
      "Epoch 61/120, Loss: 0.5214, Accuracy: 0.6968\n",
      "Epoch 62/120, Loss: 0.5212, Accuracy: 0.6966\n",
      "Epoch 63/120, Loss: 0.5210, Accuracy: 0.6971\n",
      "Epoch 64/120, Loss: 0.5211, Accuracy: 0.6969\n",
      "Epoch 65/120, Loss: 0.5211, Accuracy: 0.6971\n",
      "Epoch 66/120, Loss: 0.5212, Accuracy: 0.6967\n",
      "Epoch 67/120, Loss: 0.5210, Accuracy: 0.6973\n",
      "Epoch 68/120, Loss: 0.5211, Accuracy: 0.6977\n",
      "Epoch 69/120, Loss: 0.5213, Accuracy: 0.6978\n",
      "Epoch 70/120, Loss: 0.5211, Accuracy: 0.6969\n",
      "Epoch 71/120, Loss: 0.5211, Accuracy: 0.6973\n",
      "Epoch 72/120, Loss: 0.5211, Accuracy: 0.6966\n",
      "Epoch 73/120, Loss: 0.5210, Accuracy: 0.6977\n",
      "Epoch 74/120, Loss: 0.5211, Accuracy: 0.6970\n",
      "Epoch 75/120, Loss: 0.5210, Accuracy: 0.6973\n",
      "Epoch 76/120, Loss: 0.5211, Accuracy: 0.6964\n",
      "Epoch 77/120, Loss: 0.5210, Accuracy: 0.6968\n",
      "Epoch 78/120, Loss: 0.5209, Accuracy: 0.6970\n",
      "Epoch 79/120, Loss: 0.5211, Accuracy: 0.6974\n",
      "Epoch 80/120, Loss: 0.5210, Accuracy: 0.6969\n",
      "Epoch 81/120, Loss: 0.5210, Accuracy: 0.6976\n",
      "Epoch 82/120, Loss: 0.5211, Accuracy: 0.6969\n",
      "Epoch 83/120, Loss: 0.5209, Accuracy: 0.6977\n",
      "Epoch 84/120, Loss: 0.5210, Accuracy: 0.6975\n",
      "Epoch 85/120, Loss: 0.5209, Accuracy: 0.6976\n",
      "Epoch 86/120, Loss: 0.5210, Accuracy: 0.6975\n",
      "Epoch 87/120, Loss: 0.5210, Accuracy: 0.6973\n",
      "Epoch 88/120, Loss: 0.5208, Accuracy: 0.6976\n",
      "Epoch 89/120, Loss: 0.5207, Accuracy: 0.6973\n",
      "Epoch 90/120, Loss: 0.5210, Accuracy: 0.6978\n",
      "Epoch 91/120, Loss: 0.5209, Accuracy: 0.6965\n",
      "Epoch 92/120, Loss: 0.5208, Accuracy: 0.6975\n",
      "Epoch 93/120, Loss: 0.5207, Accuracy: 0.6969\n",
      "Epoch 94/120, Loss: 0.5209, Accuracy: 0.6976\n",
      "Epoch 95/120, Loss: 0.5208, Accuracy: 0.6979\n",
      "Epoch 96/120, Loss: 0.5208, Accuracy: 0.6977\n",
      "Epoch 97/120, Loss: 0.5210, Accuracy: 0.6966\n",
      "Epoch 98/120, Loss: 0.5210, Accuracy: 0.6969\n",
      "Epoch 99/120, Loss: 0.5210, Accuracy: 0.6976\n",
      "Epoch 100/120, Loss: 0.5208, Accuracy: 0.6975\n",
      "Epoch 101/120, Loss: 0.5211, Accuracy: 0.6973\n",
      "Epoch 102/120, Loss: 0.5211, Accuracy: 0.6976\n",
      "Epoch 103/120, Loss: 0.5209, Accuracy: 0.6978\n",
      "Epoch 104/120, Loss: 0.5209, Accuracy: 0.6967\n",
      "Epoch 105/120, Loss: 0.5210, Accuracy: 0.6972\n",
      "Epoch 106/120, Loss: 0.5211, Accuracy: 0.6965\n",
      "Epoch 107/120, Loss: 0.5208, Accuracy: 0.6972\n",
      "Epoch 108/120, Loss: 0.5209, Accuracy: 0.6980\n",
      "Epoch 109/120, Loss: 0.5209, Accuracy: 0.6968\n",
      "Epoch 110/120, Loss: 0.5208, Accuracy: 0.6973\n",
      "Epoch 111/120, Loss: 0.5210, Accuracy: 0.6968\n",
      "Epoch 112/120, Loss: 0.5209, Accuracy: 0.6979\n",
      "Epoch 113/120, Loss: 0.5209, Accuracy: 0.6976\n",
      "Epoch 114/120, Loss: 0.5210, Accuracy: 0.6975\n",
      "Epoch 115/120, Loss: 0.5208, Accuracy: 0.6978\n",
      "Epoch 116/120, Loss: 0.5210, Accuracy: 0.6972\n",
      "Epoch 117/120, Loss: 0.5208, Accuracy: 0.6976\n",
      "Epoch 118/120, Loss: 0.5209, Accuracy: 0.6974\n",
      "Epoch 119/120, Loss: 0.5209, Accuracy: 0.6967\n",
      "Epoch 120/120, Loss: 0.5211, Accuracy: 0.6977\n",
      "Training complete!\n",
      "Accuracy: 0.6738\n",
      "Epoch 1/120, Loss: 0.5370, Accuracy: 0.6862\n",
      "Epoch 2/120, Loss: 0.5295, Accuracy: 0.6893\n",
      "Epoch 3/120, Loss: 0.5281, Accuracy: 0.6899\n",
      "Epoch 4/120, Loss: 0.5270, Accuracy: 0.6903\n",
      "Epoch 5/120, Loss: 0.5263, Accuracy: 0.6913\n",
      "Epoch 6/120, Loss: 0.5255, Accuracy: 0.6918\n",
      "Epoch 7/120, Loss: 0.5250, Accuracy: 0.6920\n",
      "Epoch 8/120, Loss: 0.5245, Accuracy: 0.6925\n",
      "Epoch 9/120, Loss: 0.5241, Accuracy: 0.6933\n",
      "Epoch 10/120, Loss: 0.5239, Accuracy: 0.6936\n",
      "Epoch 11/120, Loss: 0.5235, Accuracy: 0.6937\n",
      "Epoch 12/120, Loss: 0.5232, Accuracy: 0.6945\n",
      "Epoch 13/120, Loss: 0.5228, Accuracy: 0.6941\n",
      "Epoch 14/120, Loss: 0.5226, Accuracy: 0.6941\n",
      "Epoch 15/120, Loss: 0.5224, Accuracy: 0.6949\n",
      "Epoch 16/120, Loss: 0.5221, Accuracy: 0.6956\n",
      "Epoch 17/120, Loss: 0.5219, Accuracy: 0.6951\n",
      "Epoch 18/120, Loss: 0.5215, Accuracy: 0.6956\n",
      "Epoch 19/120, Loss: 0.5216, Accuracy: 0.6963\n",
      "Epoch 20/120, Loss: 0.5212, Accuracy: 0.6961\n",
      "Epoch 21/120, Loss: 0.5210, Accuracy: 0.6963\n",
      "Epoch 22/120, Loss: 0.5209, Accuracy: 0.6967\n",
      "Epoch 23/120, Loss: 0.5206, Accuracy: 0.6974\n",
      "Epoch 24/120, Loss: 0.5204, Accuracy: 0.6979\n",
      "Epoch 25/120, Loss: 0.5203, Accuracy: 0.6977\n",
      "Epoch 26/120, Loss: 0.5201, Accuracy: 0.6972\n",
      "Epoch 27/120, Loss: 0.5201, Accuracy: 0.6978\n",
      "Epoch 28/120, Loss: 0.5199, Accuracy: 0.6982\n",
      "Epoch 29/120, Loss: 0.5197, Accuracy: 0.6985\n",
      "Epoch 30/120, Loss: 0.5195, Accuracy: 0.6980\n",
      "Epoch 31/120, Loss: 0.5195, Accuracy: 0.6987\n",
      "Epoch 32/120, Loss: 0.5193, Accuracy: 0.6978\n",
      "Epoch 33/120, Loss: 0.5193, Accuracy: 0.6986\n",
      "Epoch 34/120, Loss: 0.5190, Accuracy: 0.6989\n",
      "Epoch 35/120, Loss: 0.5190, Accuracy: 0.6991\n",
      "Epoch 36/120, Loss: 0.5190, Accuracy: 0.6990\n",
      "Epoch 37/120, Loss: 0.5188, Accuracy: 0.6991\n",
      "Epoch 38/120, Loss: 0.5188, Accuracy: 0.6990\n",
      "Epoch 39/120, Loss: 0.5187, Accuracy: 0.6987\n",
      "Epoch 40/120, Loss: 0.5186, Accuracy: 0.6989\n",
      "Epoch 41/120, Loss: 0.5185, Accuracy: 0.6991\n",
      "Epoch 42/120, Loss: 0.5184, Accuracy: 0.6993\n",
      "Epoch 43/120, Loss: 0.5182, Accuracy: 0.6995\n",
      "Epoch 44/120, Loss: 0.5182, Accuracy: 0.6998\n",
      "Epoch 45/120, Loss: 0.5179, Accuracy: 0.6999\n",
      "Epoch 46/120, Loss: 0.5180, Accuracy: 0.7000\n",
      "Epoch 47/120, Loss: 0.5180, Accuracy: 0.6997\n",
      "Epoch 48/120, Loss: 0.5178, Accuracy: 0.7000\n",
      "Epoch 49/120, Loss: 0.5178, Accuracy: 0.7001\n",
      "Epoch 50/120, Loss: 0.5177, Accuracy: 0.7004\n",
      "Epoch 51/120, Loss: 0.5176, Accuracy: 0.7004\n",
      "Epoch 52/120, Loss: 0.5176, Accuracy: 0.7004\n",
      "Epoch 53/120, Loss: 0.5176, Accuracy: 0.7001\n",
      "Epoch 54/120, Loss: 0.5174, Accuracy: 0.7000\n",
      "Epoch 55/120, Loss: 0.5173, Accuracy: 0.7006\n",
      "Epoch 56/120, Loss: 0.5173, Accuracy: 0.7008\n",
      "Epoch 57/120, Loss: 0.5172, Accuracy: 0.7003\n",
      "Epoch 58/120, Loss: 0.5171, Accuracy: 0.7004\n",
      "Epoch 59/120, Loss: 0.5171, Accuracy: 0.7003\n",
      "Epoch 60/120, Loss: 0.5170, Accuracy: 0.7009\n",
      "Epoch 61/120, Loss: 0.5170, Accuracy: 0.7007\n",
      "Epoch 62/120, Loss: 0.5170, Accuracy: 0.7007\n",
      "Epoch 63/120, Loss: 0.5167, Accuracy: 0.7011\n",
      "Epoch 64/120, Loss: 0.5168, Accuracy: 0.7002\n",
      "Epoch 65/120, Loss: 0.5168, Accuracy: 0.7008\n",
      "Epoch 66/120, Loss: 0.5166, Accuracy: 0.7013\n",
      "Epoch 67/120, Loss: 0.5167, Accuracy: 0.7008\n",
      "Epoch 68/120, Loss: 0.5167, Accuracy: 0.7010\n",
      "Epoch 69/120, Loss: 0.5165, Accuracy: 0.7010\n",
      "Epoch 70/120, Loss: 0.5165, Accuracy: 0.7005\n",
      "Epoch 71/120, Loss: 0.5165, Accuracy: 0.7011\n",
      "Epoch 72/120, Loss: 0.5164, Accuracy: 0.7010\n",
      "Epoch 73/120, Loss: 0.5162, Accuracy: 0.7012\n",
      "Epoch 74/120, Loss: 0.5164, Accuracy: 0.7018\n",
      "Epoch 75/120, Loss: 0.5163, Accuracy: 0.7012\n",
      "Epoch 76/120, Loss: 0.5162, Accuracy: 0.7018\n",
      "Epoch 77/120, Loss: 0.5163, Accuracy: 0.7010\n",
      "Epoch 78/120, Loss: 0.5161, Accuracy: 0.7016\n",
      "Epoch 79/120, Loss: 0.5162, Accuracy: 0.7018\n",
      "Epoch 80/120, Loss: 0.5161, Accuracy: 0.7018\n",
      "Epoch 81/120, Loss: 0.5159, Accuracy: 0.7019\n",
      "Epoch 82/120, Loss: 0.5159, Accuracy: 0.7017\n",
      "Epoch 83/120, Loss: 0.5160, Accuracy: 0.7016\n",
      "Epoch 84/120, Loss: 0.5159, Accuracy: 0.7013\n",
      "Epoch 85/120, Loss: 0.5159, Accuracy: 0.7026\n",
      "Epoch 86/120, Loss: 0.5159, Accuracy: 0.7015\n",
      "Epoch 87/120, Loss: 0.5158, Accuracy: 0.7016\n",
      "Epoch 88/120, Loss: 0.5158, Accuracy: 0.7016\n",
      "Epoch 89/120, Loss: 0.5157, Accuracy: 0.7020\n",
      "Epoch 90/120, Loss: 0.5158, Accuracy: 0.7022\n",
      "Epoch 91/120, Loss: 0.5158, Accuracy: 0.7017\n",
      "Epoch 92/120, Loss: 0.5158, Accuracy: 0.7021\n",
      "Epoch 93/120, Loss: 0.5157, Accuracy: 0.7020\n",
      "Epoch 94/120, Loss: 0.5156, Accuracy: 0.7020\n",
      "Epoch 95/120, Loss: 0.5156, Accuracy: 0.7013\n",
      "Epoch 96/120, Loss: 0.5155, Accuracy: 0.7014\n",
      "Epoch 97/120, Loss: 0.5156, Accuracy: 0.7012\n",
      "Epoch 98/120, Loss: 0.5156, Accuracy: 0.7020\n",
      "Epoch 99/120, Loss: 0.5154, Accuracy: 0.7019\n",
      "Epoch 100/120, Loss: 0.5155, Accuracy: 0.7020\n",
      "Epoch 101/120, Loss: 0.5154, Accuracy: 0.7017\n",
      "Epoch 102/120, Loss: 0.5156, Accuracy: 0.7019\n",
      "Epoch 103/120, Loss: 0.5156, Accuracy: 0.7016\n",
      "Epoch 104/120, Loss: 0.5153, Accuracy: 0.7021\n",
      "Epoch 105/120, Loss: 0.5154, Accuracy: 0.7020\n",
      "Epoch 106/120, Loss: 0.5154, Accuracy: 0.7023\n",
      "Epoch 107/120, Loss: 0.5152, Accuracy: 0.7021\n",
      "Epoch 108/120, Loss: 0.5154, Accuracy: 0.7020\n",
      "Epoch 109/120, Loss: 0.5153, Accuracy: 0.7025\n",
      "Epoch 110/120, Loss: 0.5152, Accuracy: 0.7019\n",
      "Epoch 111/120, Loss: 0.5152, Accuracy: 0.7025\n",
      "Epoch 112/120, Loss: 0.5153, Accuracy: 0.7020\n",
      "Epoch 113/120, Loss: 0.5152, Accuracy: 0.7025\n",
      "Epoch 114/120, Loss: 0.5152, Accuracy: 0.7019\n",
      "Epoch 115/120, Loss: 0.5151, Accuracy: 0.7027\n",
      "Epoch 116/120, Loss: 0.5152, Accuracy: 0.7019\n",
      "Epoch 117/120, Loss: 0.5151, Accuracy: 0.7027\n",
      "Epoch 118/120, Loss: 0.5152, Accuracy: 0.7024\n",
      "Epoch 119/120, Loss: 0.5152, Accuracy: 0.7025\n",
      "Epoch 120/120, Loss: 0.5151, Accuracy: 0.7019\n",
      "Training complete!\n",
      "Accuracy: 0.6828\n",
      "{8: 0.6883071143460906, 16: 0.6737588652482269, 32: 0.6827969711310933}\n",
      "Epoch 1/120, Loss: 0.4487, Accuracy: 0.7381\n",
      "Epoch 2/120, Loss: 0.4434, Accuracy: 0.7401\n",
      "Epoch 3/120, Loss: 0.4422, Accuracy: 0.7406\n",
      "Epoch 4/120, Loss: 0.4411, Accuracy: 0.7408\n",
      "Epoch 5/120, Loss: 0.4404, Accuracy: 0.7406\n",
      "Epoch 6/120, Loss: 0.4398, Accuracy: 0.7419\n",
      "Epoch 7/120, Loss: 0.4394, Accuracy: 0.7427\n",
      "Epoch 8/120, Loss: 0.4388, Accuracy: 0.7423\n",
      "Epoch 9/120, Loss: 0.4386, Accuracy: 0.7429\n",
      "Epoch 10/120, Loss: 0.4382, Accuracy: 0.7425\n",
      "Epoch 11/120, Loss: 0.4379, Accuracy: 0.7431\n",
      "Epoch 12/120, Loss: 0.4378, Accuracy: 0.7430\n",
      "Epoch 13/120, Loss: 0.4375, Accuracy: 0.7430\n",
      "Epoch 14/120, Loss: 0.4371, Accuracy: 0.7429\n",
      "Epoch 15/120, Loss: 0.4370, Accuracy: 0.7436\n",
      "Epoch 16/120, Loss: 0.4371, Accuracy: 0.7430\n",
      "Epoch 17/120, Loss: 0.4369, Accuracy: 0.7434\n",
      "Epoch 18/120, Loss: 0.4369, Accuracy: 0.7430\n",
      "Epoch 19/120, Loss: 0.4370, Accuracy: 0.7439\n",
      "Epoch 20/120, Loss: 0.4367, Accuracy: 0.7426\n",
      "Epoch 21/120, Loss: 0.4366, Accuracy: 0.7433\n",
      "Epoch 22/120, Loss: 0.4370, Accuracy: 0.7423\n",
      "Epoch 23/120, Loss: 0.4370, Accuracy: 0.7418\n",
      "Epoch 24/120, Loss: 0.4372, Accuracy: 0.7430\n",
      "Epoch 25/120, Loss: 0.4367, Accuracy: 0.7428\n",
      "Epoch 26/120, Loss: 0.4369, Accuracy: 0.7441\n",
      "Epoch 27/120, Loss: 0.4365, Accuracy: 0.7420\n",
      "Epoch 28/120, Loss: 0.4365, Accuracy: 0.7425\n",
      "Epoch 29/120, Loss: 0.4363, Accuracy: 0.7424\n",
      "Epoch 30/120, Loss: 0.4364, Accuracy: 0.7439\n",
      "Epoch 31/120, Loss: 0.4365, Accuracy: 0.7419\n",
      "Epoch 32/120, Loss: 0.4364, Accuracy: 0.7431\n",
      "Epoch 33/120, Loss: 0.4368, Accuracy: 0.7433\n",
      "Epoch 34/120, Loss: 0.4366, Accuracy: 0.7421\n",
      "Epoch 35/120, Loss: 0.4362, Accuracy: 0.7425\n",
      "Epoch 36/120, Loss: 0.4364, Accuracy: 0.7423\n",
      "Epoch 37/120, Loss: 0.4361, Accuracy: 0.7413\n",
      "Epoch 38/120, Loss: 0.4364, Accuracy: 0.7418\n",
      "Epoch 39/120, Loss: 0.4365, Accuracy: 0.7423\n",
      "Epoch 40/120, Loss: 0.4365, Accuracy: 0.7424\n",
      "Epoch 41/120, Loss: 0.4365, Accuracy: 0.7418\n",
      "Epoch 42/120, Loss: 0.4364, Accuracy: 0.7425\n",
      "Epoch 43/120, Loss: 0.4364, Accuracy: 0.7421\n",
      "Epoch 44/120, Loss: 0.4364, Accuracy: 0.7416\n",
      "Epoch 45/120, Loss: 0.4362, Accuracy: 0.7418\n",
      "Epoch 46/120, Loss: 0.4364, Accuracy: 0.7415\n",
      "Epoch 47/120, Loss: 0.4362, Accuracy: 0.7409\n",
      "Epoch 48/120, Loss: 0.4367, Accuracy: 0.7425\n",
      "Epoch 49/120, Loss: 0.4364, Accuracy: 0.7413\n",
      "Epoch 50/120, Loss: 0.4365, Accuracy: 0.7412\n",
      "Epoch 51/120, Loss: 0.4366, Accuracy: 0.7422\n",
      "Epoch 52/120, Loss: 0.4364, Accuracy: 0.7419\n",
      "Epoch 53/120, Loss: 0.4362, Accuracy: 0.7419\n",
      "Epoch 54/120, Loss: 0.4364, Accuracy: 0.7415\n",
      "Epoch 55/120, Loss: 0.4365, Accuracy: 0.7411\n",
      "Epoch 56/120, Loss: 0.4365, Accuracy: 0.7431\n",
      "Epoch 57/120, Loss: 0.4366, Accuracy: 0.7421\n",
      "Epoch 58/120, Loss: 0.4364, Accuracy: 0.7414\n",
      "Epoch 59/120, Loss: 0.4363, Accuracy: 0.7420\n",
      "Epoch 60/120, Loss: 0.4365, Accuracy: 0.7412\n",
      "Epoch 61/120, Loss: 0.4364, Accuracy: 0.7415\n",
      "Epoch 62/120, Loss: 0.4366, Accuracy: 0.7410\n",
      "Epoch 63/120, Loss: 0.4364, Accuracy: 0.7398\n",
      "Epoch 64/120, Loss: 0.4363, Accuracy: 0.7419\n",
      "Epoch 65/120, Loss: 0.4365, Accuracy: 0.7411\n",
      "Epoch 66/120, Loss: 0.4364, Accuracy: 0.7414\n",
      "Epoch 67/120, Loss: 0.4364, Accuracy: 0.7408\n",
      "Epoch 68/120, Loss: 0.4366, Accuracy: 0.7407\n",
      "Epoch 69/120, Loss: 0.4366, Accuracy: 0.7400\n",
      "Epoch 70/120, Loss: 0.4364, Accuracy: 0.7409\n",
      "Epoch 71/120, Loss: 0.4362, Accuracy: 0.7402\n",
      "Epoch 72/120, Loss: 0.4364, Accuracy: 0.7405\n",
      "Epoch 73/120, Loss: 0.4361, Accuracy: 0.7413\n",
      "Epoch 74/120, Loss: 0.4365, Accuracy: 0.7415\n",
      "Epoch 75/120, Loss: 0.4366, Accuracy: 0.7403\n",
      "Epoch 76/120, Loss: 0.4366, Accuracy: 0.7402\n",
      "Epoch 77/120, Loss: 0.4366, Accuracy: 0.7399\n",
      "Epoch 78/120, Loss: 0.4367, Accuracy: 0.7403\n",
      "Epoch 79/120, Loss: 0.4368, Accuracy: 0.7405\n",
      "Epoch 80/120, Loss: 0.4366, Accuracy: 0.7398\n",
      "Epoch 81/120, Loss: 0.4366, Accuracy: 0.7400\n",
      "Epoch 82/120, Loss: 0.4364, Accuracy: 0.7399\n",
      "Epoch 83/120, Loss: 0.4368, Accuracy: 0.7406\n",
      "Epoch 84/120, Loss: 0.4370, Accuracy: 0.7401\n",
      "Epoch 85/120, Loss: 0.4370, Accuracy: 0.7393\n",
      "Epoch 86/120, Loss: 0.4373, Accuracy: 0.7391\n",
      "Epoch 87/120, Loss: 0.4370, Accuracy: 0.7407\n",
      "Epoch 88/120, Loss: 0.4368, Accuracy: 0.7396\n",
      "Epoch 89/120, Loss: 0.4369, Accuracy: 0.7396\n",
      "Epoch 90/120, Loss: 0.4369, Accuracy: 0.7396\n",
      "Epoch 91/120, Loss: 0.4370, Accuracy: 0.7403\n",
      "Epoch 92/120, Loss: 0.4368, Accuracy: 0.7396\n",
      "Epoch 93/120, Loss: 0.4368, Accuracy: 0.7405\n",
      "Epoch 94/120, Loss: 0.4377, Accuracy: 0.7403\n",
      "Epoch 95/120, Loss: 0.4372, Accuracy: 0.7398\n",
      "Epoch 96/120, Loss: 0.4371, Accuracy: 0.7391\n",
      "Epoch 97/120, Loss: 0.4370, Accuracy: 0.7402\n",
      "Epoch 98/120, Loss: 0.4369, Accuracy: 0.7393\n",
      "Epoch 99/120, Loss: 0.4374, Accuracy: 0.7402\n",
      "Epoch 100/120, Loss: 0.4372, Accuracy: 0.7400\n",
      "Epoch 101/120, Loss: 0.4369, Accuracy: 0.7402\n",
      "Epoch 102/120, Loss: 0.4373, Accuracy: 0.7387\n",
      "Epoch 103/120, Loss: 0.4371, Accuracy: 0.7382\n",
      "Epoch 104/120, Loss: 0.4369, Accuracy: 0.7416\n",
      "Epoch 105/120, Loss: 0.4371, Accuracy: 0.7386\n",
      "Epoch 106/120, Loss: 0.4370, Accuracy: 0.7405\n",
      "Epoch 107/120, Loss: 0.4369, Accuracy: 0.7393\n",
      "Epoch 108/120, Loss: 0.4372, Accuracy: 0.7393\n",
      "Epoch 109/120, Loss: 0.4370, Accuracy: 0.7401\n",
      "Epoch 110/120, Loss: 0.4373, Accuracy: 0.7383\n",
      "Epoch 111/120, Loss: 0.4374, Accuracy: 0.7394\n",
      "Epoch 112/120, Loss: 0.4369, Accuracy: 0.7392\n",
      "Epoch 113/120, Loss: 0.4370, Accuracy: 0.7385\n",
      "Epoch 114/120, Loss: 0.4373, Accuracy: 0.7391\n",
      "Epoch 115/120, Loss: 0.4372, Accuracy: 0.7396\n",
      "Epoch 116/120, Loss: 0.4374, Accuracy: 0.7392\n",
      "Epoch 117/120, Loss: 0.4374, Accuracy: 0.7381\n",
      "Epoch 118/120, Loss: 0.4371, Accuracy: 0.7380\n",
      "Epoch 119/120, Loss: 0.4378, Accuracy: 0.7396\n",
      "Epoch 120/120, Loss: 0.4370, Accuracy: 0.7386\n",
      "Training complete!\n",
      "Accuracy: 0.7304\n",
      "Epoch 1/120, Loss: 0.4494, Accuracy: 0.7385\n",
      "Epoch 2/120, Loss: 0.4429, Accuracy: 0.7402\n",
      "Epoch 3/120, Loss: 0.4416, Accuracy: 0.7415\n",
      "Epoch 4/120, Loss: 0.4405, Accuracy: 0.7414\n",
      "Epoch 5/120, Loss: 0.4396, Accuracy: 0.7429\n",
      "Epoch 6/120, Loss: 0.4388, Accuracy: 0.7441\n",
      "Epoch 7/120, Loss: 0.4381, Accuracy: 0.7448\n",
      "Epoch 8/120, Loss: 0.4375, Accuracy: 0.7451\n",
      "Epoch 9/120, Loss: 0.4370, Accuracy: 0.7454\n",
      "Epoch 10/120, Loss: 0.4363, Accuracy: 0.7452\n",
      "Epoch 11/120, Loss: 0.4358, Accuracy: 0.7458\n",
      "Epoch 12/120, Loss: 0.4356, Accuracy: 0.7463\n",
      "Epoch 13/120, Loss: 0.4351, Accuracy: 0.7465\n",
      "Epoch 14/120, Loss: 0.4349, Accuracy: 0.7476\n",
      "Epoch 15/120, Loss: 0.4346, Accuracy: 0.7470\n",
      "Epoch 16/120, Loss: 0.4344, Accuracy: 0.7474\n",
      "Epoch 17/120, Loss: 0.4341, Accuracy: 0.7477\n",
      "Epoch 18/120, Loss: 0.4339, Accuracy: 0.7478\n",
      "Epoch 19/120, Loss: 0.4337, Accuracy: 0.7481\n",
      "Epoch 20/120, Loss: 0.4333, Accuracy: 0.7486\n",
      "Epoch 21/120, Loss: 0.4331, Accuracy: 0.7484\n",
      "Epoch 22/120, Loss: 0.4331, Accuracy: 0.7485\n",
      "Epoch 23/120, Loss: 0.4328, Accuracy: 0.7489\n",
      "Epoch 24/120, Loss: 0.4328, Accuracy: 0.7495\n",
      "Epoch 25/120, Loss: 0.4324, Accuracy: 0.7498\n",
      "Epoch 26/120, Loss: 0.4325, Accuracy: 0.7492\n",
      "Epoch 27/120, Loss: 0.4324, Accuracy: 0.7489\n",
      "Epoch 28/120, Loss: 0.4323, Accuracy: 0.7491\n",
      "Epoch 29/120, Loss: 0.4321, Accuracy: 0.7497\n",
      "Epoch 30/120, Loss: 0.4318, Accuracy: 0.7502\n",
      "Epoch 31/120, Loss: 0.4319, Accuracy: 0.7502\n",
      "Epoch 32/120, Loss: 0.4319, Accuracy: 0.7505\n",
      "Epoch 33/120, Loss: 0.4317, Accuracy: 0.7503\n",
      "Epoch 34/120, Loss: 0.4318, Accuracy: 0.7497\n",
      "Epoch 35/120, Loss: 0.4318, Accuracy: 0.7503\n",
      "Epoch 36/120, Loss: 0.4315, Accuracy: 0.7497\n",
      "Epoch 37/120, Loss: 0.4316, Accuracy: 0.7498\n",
      "Epoch 38/120, Loss: 0.4314, Accuracy: 0.7505\n",
      "Epoch 39/120, Loss: 0.4314, Accuracy: 0.7499\n",
      "Epoch 40/120, Loss: 0.4314, Accuracy: 0.7497\n",
      "Epoch 41/120, Loss: 0.4313, Accuracy: 0.7498\n",
      "Epoch 42/120, Loss: 0.4316, Accuracy: 0.7503\n",
      "Epoch 43/120, Loss: 0.4313, Accuracy: 0.7505\n",
      "Epoch 44/120, Loss: 0.4312, Accuracy: 0.7502\n",
      "Epoch 45/120, Loss: 0.4311, Accuracy: 0.7505\n",
      "Epoch 46/120, Loss: 0.4309, Accuracy: 0.7505\n",
      "Epoch 47/120, Loss: 0.4309, Accuracy: 0.7509\n",
      "Epoch 48/120, Loss: 0.4311, Accuracy: 0.7505\n",
      "Epoch 49/120, Loss: 0.4310, Accuracy: 0.7500\n",
      "Epoch 50/120, Loss: 0.4310, Accuracy: 0.7508\n",
      "Epoch 51/120, Loss: 0.4310, Accuracy: 0.7507\n",
      "Epoch 52/120, Loss: 0.4309, Accuracy: 0.7505\n",
      "Epoch 53/120, Loss: 0.4311, Accuracy: 0.7502\n",
      "Epoch 54/120, Loss: 0.4305, Accuracy: 0.7510\n",
      "Epoch 55/120, Loss: 0.4308, Accuracy: 0.7509\n",
      "Epoch 56/120, Loss: 0.4308, Accuracy: 0.7502\n",
      "Epoch 57/120, Loss: 0.4310, Accuracy: 0.7511\n",
      "Epoch 58/120, Loss: 0.4311, Accuracy: 0.7502\n",
      "Epoch 59/120, Loss: 0.4310, Accuracy: 0.7501\n",
      "Epoch 60/120, Loss: 0.4307, Accuracy: 0.7505\n",
      "Epoch 61/120, Loss: 0.4307, Accuracy: 0.7506\n",
      "Epoch 62/120, Loss: 0.4311, Accuracy: 0.7499\n",
      "Epoch 63/120, Loss: 0.4307, Accuracy: 0.7509\n",
      "Epoch 64/120, Loss: 0.4307, Accuracy: 0.7511\n",
      "Epoch 65/120, Loss: 0.4306, Accuracy: 0.7514\n",
      "Epoch 66/120, Loss: 0.4306, Accuracy: 0.7502\n",
      "Epoch 67/120, Loss: 0.4307, Accuracy: 0.7510\n",
      "Epoch 68/120, Loss: 0.4305, Accuracy: 0.7507\n",
      "Epoch 69/120, Loss: 0.4304, Accuracy: 0.7502\n",
      "Epoch 70/120, Loss: 0.4304, Accuracy: 0.7505\n",
      "Epoch 71/120, Loss: 0.4303, Accuracy: 0.7505\n",
      "Epoch 72/120, Loss: 0.4307, Accuracy: 0.7507\n",
      "Epoch 73/120, Loss: 0.4306, Accuracy: 0.7504\n",
      "Epoch 74/120, Loss: 0.4305, Accuracy: 0.7502\n",
      "Epoch 75/120, Loss: 0.4304, Accuracy: 0.7505\n",
      "Epoch 76/120, Loss: 0.4304, Accuracy: 0.7504\n",
      "Epoch 77/120, Loss: 0.4304, Accuracy: 0.7508\n",
      "Epoch 78/120, Loss: 0.4304, Accuracy: 0.7507\n",
      "Epoch 79/120, Loss: 0.4303, Accuracy: 0.7506\n",
      "Epoch 80/120, Loss: 0.4304, Accuracy: 0.7502\n",
      "Epoch 81/120, Loss: 0.4303, Accuracy: 0.7504\n",
      "Epoch 82/120, Loss: 0.4302, Accuracy: 0.7509\n",
      "Epoch 83/120, Loss: 0.4303, Accuracy: 0.7509\n",
      "Epoch 84/120, Loss: 0.4305, Accuracy: 0.7502\n",
      "Epoch 85/120, Loss: 0.4304, Accuracy: 0.7504\n",
      "Epoch 86/120, Loss: 0.4305, Accuracy: 0.7497\n",
      "Epoch 87/120, Loss: 0.4305, Accuracy: 0.7505\n",
      "Epoch 88/120, Loss: 0.4304, Accuracy: 0.7510\n",
      "Epoch 89/120, Loss: 0.4303, Accuracy: 0.7505\n",
      "Epoch 90/120, Loss: 0.4305, Accuracy: 0.7501\n",
      "Epoch 91/120, Loss: 0.4304, Accuracy: 0.7503\n",
      "Epoch 92/120, Loss: 0.4303, Accuracy: 0.7504\n",
      "Epoch 93/120, Loss: 0.4304, Accuracy: 0.7494\n",
      "Epoch 94/120, Loss: 0.4305, Accuracy: 0.7502\n",
      "Epoch 95/120, Loss: 0.4304, Accuracy: 0.7506\n",
      "Epoch 96/120, Loss: 0.4306, Accuracy: 0.7506\n",
      "Epoch 97/120, Loss: 0.4305, Accuracy: 0.7498\n",
      "Epoch 98/120, Loss: 0.4304, Accuracy: 0.7502\n",
      "Epoch 99/120, Loss: 0.4303, Accuracy: 0.7515\n",
      "Epoch 100/120, Loss: 0.4304, Accuracy: 0.7507\n",
      "Epoch 101/120, Loss: 0.4303, Accuracy: 0.7502\n",
      "Epoch 102/120, Loss: 0.4305, Accuracy: 0.7508\n",
      "Epoch 103/120, Loss: 0.4304, Accuracy: 0.7507\n",
      "Epoch 104/120, Loss: 0.4304, Accuracy: 0.7498\n",
      "Epoch 105/120, Loss: 0.4305, Accuracy: 0.7508\n",
      "Epoch 106/120, Loss: 0.4303, Accuracy: 0.7507\n",
      "Epoch 107/120, Loss: 0.4304, Accuracy: 0.7501\n",
      "Epoch 108/120, Loss: 0.4303, Accuracy: 0.7505\n",
      "Epoch 109/120, Loss: 0.4305, Accuracy: 0.7505\n",
      "Epoch 110/120, Loss: 0.4302, Accuracy: 0.7500\n",
      "Epoch 111/120, Loss: 0.4306, Accuracy: 0.7504\n",
      "Epoch 112/120, Loss: 0.4304, Accuracy: 0.7500\n",
      "Epoch 113/120, Loss: 0.4304, Accuracy: 0.7501\n",
      "Epoch 114/120, Loss: 0.4308, Accuracy: 0.7498\n",
      "Epoch 115/120, Loss: 0.4304, Accuracy: 0.7504\n",
      "Epoch 116/120, Loss: 0.4307, Accuracy: 0.7503\n",
      "Epoch 117/120, Loss: 0.4305, Accuracy: 0.7501\n",
      "Epoch 118/120, Loss: 0.4305, Accuracy: 0.7495\n",
      "Epoch 119/120, Loss: 0.4304, Accuracy: 0.7504\n",
      "Epoch 120/120, Loss: 0.4306, Accuracy: 0.7508\n",
      "Training complete!\n",
      "Accuracy: 0.7356\n",
      "Epoch 1/120, Loss: 0.4502, Accuracy: 0.7378\n",
      "Epoch 2/120, Loss: 0.4427, Accuracy: 0.7405\n",
      "Epoch 3/120, Loss: 0.4411, Accuracy: 0.7422\n",
      "Epoch 4/120, Loss: 0.4398, Accuracy: 0.7428\n",
      "Epoch 5/120, Loss: 0.4391, Accuracy: 0.7424\n",
      "Epoch 6/120, Loss: 0.4381, Accuracy: 0.7434\n",
      "Epoch 7/120, Loss: 0.4372, Accuracy: 0.7438\n",
      "Epoch 8/120, Loss: 0.4367, Accuracy: 0.7450\n",
      "Epoch 9/120, Loss: 0.4360, Accuracy: 0.7455\n",
      "Epoch 10/120, Loss: 0.4353, Accuracy: 0.7459\n",
      "Epoch 11/120, Loss: 0.4348, Accuracy: 0.7468\n",
      "Epoch 12/120, Loss: 0.4342, Accuracy: 0.7470\n",
      "Epoch 13/120, Loss: 0.4336, Accuracy: 0.7475\n",
      "Epoch 14/120, Loss: 0.4333, Accuracy: 0.7481\n",
      "Epoch 15/120, Loss: 0.4328, Accuracy: 0.7484\n",
      "Epoch 16/120, Loss: 0.4323, Accuracy: 0.7484\n",
      "Epoch 17/120, Loss: 0.4320, Accuracy: 0.7489\n",
      "Epoch 18/120, Loss: 0.4317, Accuracy: 0.7495\n",
      "Epoch 19/120, Loss: 0.4311, Accuracy: 0.7493\n",
      "Epoch 20/120, Loss: 0.4308, Accuracy: 0.7496\n",
      "Epoch 21/120, Loss: 0.4304, Accuracy: 0.7503\n",
      "Epoch 22/120, Loss: 0.4301, Accuracy: 0.7505\n",
      "Epoch 23/120, Loss: 0.4298, Accuracy: 0.7500\n",
      "Epoch 24/120, Loss: 0.4295, Accuracy: 0.7508\n",
      "Epoch 25/120, Loss: 0.4292, Accuracy: 0.7512\n",
      "Epoch 26/120, Loss: 0.4287, Accuracy: 0.7513\n",
      "Epoch 27/120, Loss: 0.4285, Accuracy: 0.7516\n",
      "Epoch 28/120, Loss: 0.4283, Accuracy: 0.7523\n",
      "Epoch 29/120, Loss: 0.4278, Accuracy: 0.7518\n",
      "Epoch 30/120, Loss: 0.4275, Accuracy: 0.7523\n",
      "Epoch 31/120, Loss: 0.4273, Accuracy: 0.7519\n",
      "Epoch 32/120, Loss: 0.4272, Accuracy: 0.7523\n",
      "Epoch 33/120, Loss: 0.4269, Accuracy: 0.7528\n",
      "Epoch 34/120, Loss: 0.4265, Accuracy: 0.7529\n",
      "Epoch 35/120, Loss: 0.4263, Accuracy: 0.7529\n",
      "Epoch 36/120, Loss: 0.4261, Accuracy: 0.7534\n",
      "Epoch 37/120, Loss: 0.4260, Accuracy: 0.7527\n",
      "Epoch 38/120, Loss: 0.4259, Accuracy: 0.7536\n",
      "Epoch 39/120, Loss: 0.4257, Accuracy: 0.7534\n",
      "Epoch 40/120, Loss: 0.4255, Accuracy: 0.7535\n",
      "Epoch 41/120, Loss: 0.4255, Accuracy: 0.7538\n",
      "Epoch 42/120, Loss: 0.4252, Accuracy: 0.7541\n",
      "Epoch 43/120, Loss: 0.4250, Accuracy: 0.7540\n",
      "Epoch 44/120, Loss: 0.4250, Accuracy: 0.7541\n",
      "Epoch 45/120, Loss: 0.4246, Accuracy: 0.7544\n",
      "Epoch 46/120, Loss: 0.4245, Accuracy: 0.7546\n",
      "Epoch 47/120, Loss: 0.4244, Accuracy: 0.7541\n",
      "Epoch 48/120, Loss: 0.4241, Accuracy: 0.7544\n",
      "Epoch 49/120, Loss: 0.4241, Accuracy: 0.7547\n",
      "Epoch 50/120, Loss: 0.4241, Accuracy: 0.7546\n",
      "Epoch 51/120, Loss: 0.4238, Accuracy: 0.7547\n",
      "Epoch 52/120, Loss: 0.4237, Accuracy: 0.7550\n",
      "Epoch 53/120, Loss: 0.4237, Accuracy: 0.7553\n",
      "Epoch 54/120, Loss: 0.4233, Accuracy: 0.7556\n",
      "Epoch 55/120, Loss: 0.4235, Accuracy: 0.7558\n",
      "Epoch 56/120, Loss: 0.4234, Accuracy: 0.7560\n",
      "Epoch 57/120, Loss: 0.4234, Accuracy: 0.7558\n",
      "Epoch 58/120, Loss: 0.4233, Accuracy: 0.7556\n",
      "Epoch 59/120, Loss: 0.4230, Accuracy: 0.7559\n",
      "Epoch 60/120, Loss: 0.4231, Accuracy: 0.7551\n",
      "Epoch 61/120, Loss: 0.4228, Accuracy: 0.7560\n",
      "Epoch 62/120, Loss: 0.4230, Accuracy: 0.7564\n",
      "Epoch 63/120, Loss: 0.4228, Accuracy: 0.7559\n",
      "Epoch 64/120, Loss: 0.4227, Accuracy: 0.7563\n",
      "Epoch 65/120, Loss: 0.4226, Accuracy: 0.7563\n",
      "Epoch 66/120, Loss: 0.4227, Accuracy: 0.7557\n",
      "Epoch 67/120, Loss: 0.4226, Accuracy: 0.7567\n",
      "Epoch 68/120, Loss: 0.4223, Accuracy: 0.7566\n",
      "Epoch 69/120, Loss: 0.4223, Accuracy: 0.7565\n",
      "Epoch 70/120, Loss: 0.4222, Accuracy: 0.7570\n",
      "Epoch 71/120, Loss: 0.4223, Accuracy: 0.7561\n",
      "Epoch 72/120, Loss: 0.4221, Accuracy: 0.7567\n",
      "Epoch 73/120, Loss: 0.4219, Accuracy: 0.7562\n",
      "Epoch 74/120, Loss: 0.4219, Accuracy: 0.7568\n",
      "Epoch 75/120, Loss: 0.4219, Accuracy: 0.7563\n",
      "Epoch 76/120, Loss: 0.4218, Accuracy: 0.7573\n",
      "Epoch 77/120, Loss: 0.4217, Accuracy: 0.7570\n",
      "Epoch 78/120, Loss: 0.4217, Accuracy: 0.7570\n",
      "Epoch 79/120, Loss: 0.4216, Accuracy: 0.7569\n",
      "Epoch 80/120, Loss: 0.4215, Accuracy: 0.7576\n",
      "Epoch 81/120, Loss: 0.4215, Accuracy: 0.7572\n",
      "Epoch 82/120, Loss: 0.4216, Accuracy: 0.7566\n",
      "Epoch 83/120, Loss: 0.4214, Accuracy: 0.7572\n",
      "Epoch 84/120, Loss: 0.4214, Accuracy: 0.7568\n",
      "Epoch 85/120, Loss: 0.4214, Accuracy: 0.7566\n",
      "Epoch 86/120, Loss: 0.4212, Accuracy: 0.7578\n",
      "Epoch 87/120, Loss: 0.4211, Accuracy: 0.7569\n",
      "Epoch 88/120, Loss: 0.4211, Accuracy: 0.7570\n",
      "Epoch 89/120, Loss: 0.4211, Accuracy: 0.7577\n",
      "Epoch 90/120, Loss: 0.4211, Accuracy: 0.7575\n",
      "Epoch 91/120, Loss: 0.4210, Accuracy: 0.7578\n",
      "Epoch 92/120, Loss: 0.4209, Accuracy: 0.7578\n",
      "Epoch 93/120, Loss: 0.4211, Accuracy: 0.7581\n",
      "Epoch 94/120, Loss: 0.4209, Accuracy: 0.7575\n",
      "Epoch 95/120, Loss: 0.4208, Accuracy: 0.7574\n",
      "Epoch 96/120, Loss: 0.4210, Accuracy: 0.7577\n",
      "Epoch 97/120, Loss: 0.4208, Accuracy: 0.7580\n",
      "Epoch 98/120, Loss: 0.4208, Accuracy: 0.7572\n",
      "Epoch 99/120, Loss: 0.4207, Accuracy: 0.7575\n",
      "Epoch 100/120, Loss: 0.4206, Accuracy: 0.7573\n",
      "Epoch 101/120, Loss: 0.4206, Accuracy: 0.7579\n",
      "Epoch 102/120, Loss: 0.4208, Accuracy: 0.7575\n",
      "Epoch 103/120, Loss: 0.4207, Accuracy: 0.7575\n",
      "Epoch 104/120, Loss: 0.4206, Accuracy: 0.7584\n",
      "Epoch 105/120, Loss: 0.4205, Accuracy: 0.7580\n",
      "Epoch 106/120, Loss: 0.4204, Accuracy: 0.7579\n",
      "Epoch 107/120, Loss: 0.4204, Accuracy: 0.7577\n",
      "Epoch 108/120, Loss: 0.4205, Accuracy: 0.7576\n",
      "Epoch 109/120, Loss: 0.4203, Accuracy: 0.7577\n",
      "Epoch 110/120, Loss: 0.4204, Accuracy: 0.7580\n",
      "Epoch 111/120, Loss: 0.4203, Accuracy: 0.7582\n",
      "Epoch 112/120, Loss: 0.4203, Accuracy: 0.7579\n",
      "Epoch 113/120, Loss: 0.4201, Accuracy: 0.7582\n",
      "Epoch 114/120, Loss: 0.4200, Accuracy: 0.7579\n",
      "Epoch 115/120, Loss: 0.4202, Accuracy: 0.7585\n",
      "Epoch 116/120, Loss: 0.4204, Accuracy: 0.7580\n",
      "Epoch 117/120, Loss: 0.4203, Accuracy: 0.7582\n",
      "Epoch 118/120, Loss: 0.4200, Accuracy: 0.7581\n",
      "Epoch 119/120, Loss: 0.4202, Accuracy: 0.7577\n",
      "Epoch 120/120, Loss: 0.4199, Accuracy: 0.7582\n",
      "Training complete!\n",
      "Accuracy: 0.7242\n",
      "{8: 0.7303745827667203, 16: 0.7355874483094138, 32: 0.7241631162507608}\n",
      "Epoch 1/120, Loss: 0.4980, Accuracy: 0.7080\n",
      "Epoch 2/120, Loss: 0.4868, Accuracy: 0.7130\n",
      "Epoch 3/120, Loss: 0.4840, Accuracy: 0.7158\n",
      "Epoch 4/120, Loss: 0.4820, Accuracy: 0.7163\n",
      "Epoch 5/120, Loss: 0.4803, Accuracy: 0.7175\n",
      "Epoch 6/120, Loss: 0.4791, Accuracy: 0.7189\n",
      "Epoch 7/120, Loss: 0.4777, Accuracy: 0.7201\n",
      "Epoch 8/120, Loss: 0.4770, Accuracy: 0.7213\n",
      "Epoch 9/120, Loss: 0.4764, Accuracy: 0.7225\n",
      "Epoch 10/120, Loss: 0.4755, Accuracy: 0.7229\n",
      "Epoch 11/120, Loss: 0.4750, Accuracy: 0.7231\n",
      "Epoch 12/120, Loss: 0.4746, Accuracy: 0.7241\n",
      "Epoch 13/120, Loss: 0.4742, Accuracy: 0.7246\n",
      "Epoch 14/120, Loss: 0.4737, Accuracy: 0.7240\n",
      "Epoch 15/120, Loss: 0.4734, Accuracy: 0.7255\n",
      "Epoch 16/120, Loss: 0.4730, Accuracy: 0.7257\n",
      "Epoch 17/120, Loss: 0.4725, Accuracy: 0.7260\n",
      "Epoch 18/120, Loss: 0.4723, Accuracy: 0.7267\n",
      "Epoch 19/120, Loss: 0.4721, Accuracy: 0.7263\n",
      "Epoch 20/120, Loss: 0.4720, Accuracy: 0.7277\n",
      "Epoch 21/120, Loss: 0.4720, Accuracy: 0.7264\n",
      "Epoch 22/120, Loss: 0.4718, Accuracy: 0.7274\n",
      "Epoch 23/120, Loss: 0.4717, Accuracy: 0.7271\n",
      "Epoch 24/120, Loss: 0.4716, Accuracy: 0.7278\n",
      "Epoch 25/120, Loss: 0.4710, Accuracy: 0.7279\n",
      "Epoch 26/120, Loss: 0.4714, Accuracy: 0.7276\n",
      "Epoch 27/120, Loss: 0.4711, Accuracy: 0.7276\n",
      "Epoch 28/120, Loss: 0.4709, Accuracy: 0.7289\n",
      "Epoch 29/120, Loss: 0.4708, Accuracy: 0.7282\n",
      "Epoch 30/120, Loss: 0.4711, Accuracy: 0.7275\n",
      "Epoch 31/120, Loss: 0.4705, Accuracy: 0.7295\n",
      "Epoch 32/120, Loss: 0.4706, Accuracy: 0.7282\n",
      "Epoch 33/120, Loss: 0.4705, Accuracy: 0.7288\n",
      "Epoch 34/120, Loss: 0.4708, Accuracy: 0.7284\n",
      "Epoch 35/120, Loss: 0.4704, Accuracy: 0.7285\n",
      "Epoch 36/120, Loss: 0.4703, Accuracy: 0.7298\n",
      "Epoch 37/120, Loss: 0.4705, Accuracy: 0.7292\n",
      "Epoch 38/120, Loss: 0.4702, Accuracy: 0.7289\n",
      "Epoch 39/120, Loss: 0.4705, Accuracy: 0.7289\n",
      "Epoch 40/120, Loss: 0.4705, Accuracy: 0.7291\n",
      "Epoch 41/120, Loss: 0.4702, Accuracy: 0.7292\n",
      "Epoch 42/120, Loss: 0.4706, Accuracy: 0.7294\n",
      "Epoch 43/120, Loss: 0.4702, Accuracy: 0.7289\n",
      "Epoch 44/120, Loss: 0.4706, Accuracy: 0.7281\n",
      "Epoch 45/120, Loss: 0.4707, Accuracy: 0.7285\n",
      "Epoch 46/120, Loss: 0.4706, Accuracy: 0.7288\n",
      "Epoch 47/120, Loss: 0.4708, Accuracy: 0.7289\n",
      "Epoch 48/120, Loss: 0.4706, Accuracy: 0.7289\n",
      "Epoch 49/120, Loss: 0.4708, Accuracy: 0.7285\n",
      "Epoch 50/120, Loss: 0.4708, Accuracy: 0.7289\n",
      "Epoch 51/120, Loss: 0.4712, Accuracy: 0.7282\n",
      "Epoch 52/120, Loss: 0.4706, Accuracy: 0.7285\n",
      "Epoch 53/120, Loss: 0.4704, Accuracy: 0.7287\n",
      "Epoch 54/120, Loss: 0.4709, Accuracy: 0.7293\n",
      "Epoch 55/120, Loss: 0.4706, Accuracy: 0.7290\n",
      "Epoch 56/120, Loss: 0.4705, Accuracy: 0.7290\n",
      "Epoch 57/120, Loss: 0.4704, Accuracy: 0.7282\n",
      "Epoch 58/120, Loss: 0.4710, Accuracy: 0.7298\n",
      "Epoch 59/120, Loss: 0.4707, Accuracy: 0.7294\n",
      "Epoch 60/120, Loss: 0.4710, Accuracy: 0.7284\n",
      "Epoch 61/120, Loss: 0.4706, Accuracy: 0.7287\n",
      "Epoch 62/120, Loss: 0.4706, Accuracy: 0.7293\n",
      "Epoch 63/120, Loss: 0.4708, Accuracy: 0.7297\n",
      "Epoch 64/120, Loss: 0.4708, Accuracy: 0.7297\n",
      "Epoch 65/120, Loss: 0.4707, Accuracy: 0.7298\n",
      "Epoch 66/120, Loss: 0.4706, Accuracy: 0.7287\n",
      "Epoch 67/120, Loss: 0.4707, Accuracy: 0.7293\n",
      "Epoch 68/120, Loss: 0.4708, Accuracy: 0.7293\n",
      "Epoch 69/120, Loss: 0.4704, Accuracy: 0.7291\n",
      "Epoch 70/120, Loss: 0.4709, Accuracy: 0.7293\n",
      "Epoch 71/120, Loss: 0.4712, Accuracy: 0.7286\n",
      "Epoch 72/120, Loss: 0.4710, Accuracy: 0.7286\n",
      "Epoch 73/120, Loss: 0.4706, Accuracy: 0.7289\n",
      "Epoch 74/120, Loss: 0.4715, Accuracy: 0.7291\n",
      "Epoch 75/120, Loss: 0.4708, Accuracy: 0.7293\n",
      "Epoch 76/120, Loss: 0.4704, Accuracy: 0.7297\n",
      "Epoch 77/120, Loss: 0.4708, Accuracy: 0.7299\n",
      "Epoch 78/120, Loss: 0.4710, Accuracy: 0.7290\n",
      "Epoch 79/120, Loss: 0.4708, Accuracy: 0.7293\n",
      "Epoch 80/120, Loss: 0.4709, Accuracy: 0.7303\n",
      "Epoch 81/120, Loss: 0.4713, Accuracy: 0.7298\n",
      "Epoch 82/120, Loss: 0.4713, Accuracy: 0.7296\n",
      "Epoch 83/120, Loss: 0.4709, Accuracy: 0.7295\n",
      "Epoch 84/120, Loss: 0.4712, Accuracy: 0.7292\n",
      "Epoch 85/120, Loss: 0.4711, Accuracy: 0.7285\n",
      "Epoch 86/120, Loss: 0.4711, Accuracy: 0.7301\n",
      "Epoch 87/120, Loss: 0.4715, Accuracy: 0.7295\n",
      "Epoch 88/120, Loss: 0.4713, Accuracy: 0.7285\n",
      "Epoch 89/120, Loss: 0.4718, Accuracy: 0.7296\n",
      "Epoch 90/120, Loss: 0.4717, Accuracy: 0.7293\n",
      "Epoch 91/120, Loss: 0.4716, Accuracy: 0.7296\n",
      "Epoch 92/120, Loss: 0.4714, Accuracy: 0.7291\n",
      "Epoch 93/120, Loss: 0.4713, Accuracy: 0.7300\n",
      "Epoch 94/120, Loss: 0.4716, Accuracy: 0.7296\n",
      "Epoch 95/120, Loss: 0.4717, Accuracy: 0.7283\n",
      "Epoch 96/120, Loss: 0.4716, Accuracy: 0.7285\n",
      "Epoch 97/120, Loss: 0.4719, Accuracy: 0.7290\n",
      "Epoch 98/120, Loss: 0.4720, Accuracy: 0.7287\n",
      "Epoch 99/120, Loss: 0.4722, Accuracy: 0.7290\n",
      "Epoch 100/120, Loss: 0.4717, Accuracy: 0.7291\n",
      "Epoch 101/120, Loss: 0.4723, Accuracy: 0.7283\n",
      "Epoch 102/120, Loss: 0.4722, Accuracy: 0.7294\n",
      "Epoch 103/120, Loss: 0.4722, Accuracy: 0.7284\n",
      "Epoch 104/120, Loss: 0.4718, Accuracy: 0.7283\n",
      "Epoch 105/120, Loss: 0.4724, Accuracy: 0.7284\n",
      "Epoch 106/120, Loss: 0.4719, Accuracy: 0.7292\n",
      "Epoch 107/120, Loss: 0.4722, Accuracy: 0.7292\n",
      "Epoch 108/120, Loss: 0.4725, Accuracy: 0.7287\n",
      "Epoch 109/120, Loss: 0.4724, Accuracy: 0.7285\n",
      "Epoch 110/120, Loss: 0.4724, Accuracy: 0.7285\n",
      "Epoch 111/120, Loss: 0.4721, Accuracy: 0.7283\n",
      "Epoch 112/120, Loss: 0.4722, Accuracy: 0.7282\n",
      "Epoch 113/120, Loss: 0.4721, Accuracy: 0.7288\n",
      "Epoch 114/120, Loss: 0.4723, Accuracy: 0.7279\n",
      "Epoch 115/120, Loss: 0.4720, Accuracy: 0.7287\n",
      "Epoch 116/120, Loss: 0.4724, Accuracy: 0.7284\n",
      "Epoch 117/120, Loss: 0.4726, Accuracy: 0.7288\n",
      "Epoch 118/120, Loss: 0.4728, Accuracy: 0.7287\n",
      "Epoch 119/120, Loss: 0.4727, Accuracy: 0.7289\n",
      "Epoch 120/120, Loss: 0.4725, Accuracy: 0.7286\n",
      "Training complete!\n",
      "Accuracy: 0.7097\n",
      "Epoch 1/120, Loss: 0.4990, Accuracy: 0.7063\n",
      "Epoch 2/120, Loss: 0.4859, Accuracy: 0.7126\n",
      "Epoch 3/120, Loss: 0.4829, Accuracy: 0.7155\n",
      "Epoch 4/120, Loss: 0.4808, Accuracy: 0.7183\n",
      "Epoch 5/120, Loss: 0.4793, Accuracy: 0.7186\n",
      "Epoch 6/120, Loss: 0.4777, Accuracy: 0.7204\n",
      "Epoch 7/120, Loss: 0.4764, Accuracy: 0.7222\n",
      "Epoch 8/120, Loss: 0.4757, Accuracy: 0.7223\n",
      "Epoch 9/120, Loss: 0.4745, Accuracy: 0.7229\n",
      "Epoch 10/120, Loss: 0.4737, Accuracy: 0.7248\n",
      "Epoch 11/120, Loss: 0.4727, Accuracy: 0.7250\n",
      "Epoch 12/120, Loss: 0.4721, Accuracy: 0.7256\n",
      "Epoch 13/120, Loss: 0.4716, Accuracy: 0.7266\n",
      "Epoch 14/120, Loss: 0.4711, Accuracy: 0.7269\n",
      "Epoch 15/120, Loss: 0.4704, Accuracy: 0.7274\n",
      "Epoch 16/120, Loss: 0.4699, Accuracy: 0.7279\n",
      "Epoch 17/120, Loss: 0.4693, Accuracy: 0.7288\n",
      "Epoch 18/120, Loss: 0.4692, Accuracy: 0.7292\n",
      "Epoch 19/120, Loss: 0.4689, Accuracy: 0.7294\n",
      "Epoch 20/120, Loss: 0.4685, Accuracy: 0.7292\n",
      "Epoch 21/120, Loss: 0.4679, Accuracy: 0.7295\n",
      "Epoch 22/120, Loss: 0.4678, Accuracy: 0.7300\n",
      "Epoch 23/120, Loss: 0.4675, Accuracy: 0.7294\n",
      "Epoch 24/120, Loss: 0.4673, Accuracy: 0.7303\n",
      "Epoch 25/120, Loss: 0.4670, Accuracy: 0.7307\n",
      "Epoch 26/120, Loss: 0.4663, Accuracy: 0.7308\n",
      "Epoch 27/120, Loss: 0.4668, Accuracy: 0.7303\n",
      "Epoch 28/120, Loss: 0.4663, Accuracy: 0.7304\n",
      "Epoch 29/120, Loss: 0.4659, Accuracy: 0.7308\n",
      "Epoch 30/120, Loss: 0.4657, Accuracy: 0.7313\n",
      "Epoch 31/120, Loss: 0.4654, Accuracy: 0.7309\n",
      "Epoch 32/120, Loss: 0.4655, Accuracy: 0.7316\n",
      "Epoch 33/120, Loss: 0.4652, Accuracy: 0.7320\n",
      "Epoch 34/120, Loss: 0.4651, Accuracy: 0.7319\n",
      "Epoch 35/120, Loss: 0.4649, Accuracy: 0.7318\n",
      "Epoch 36/120, Loss: 0.4646, Accuracy: 0.7322\n",
      "Epoch 37/120, Loss: 0.4645, Accuracy: 0.7317\n",
      "Epoch 38/120, Loss: 0.4644, Accuracy: 0.7316\n",
      "Epoch 39/120, Loss: 0.4641, Accuracy: 0.7326\n",
      "Epoch 40/120, Loss: 0.4641, Accuracy: 0.7324\n",
      "Epoch 41/120, Loss: 0.4638, Accuracy: 0.7326\n",
      "Epoch 42/120, Loss: 0.4638, Accuracy: 0.7324\n",
      "Epoch 43/120, Loss: 0.4633, Accuracy: 0.7320\n",
      "Epoch 44/120, Loss: 0.4634, Accuracy: 0.7331\n",
      "Epoch 45/120, Loss: 0.4633, Accuracy: 0.7330\n",
      "Epoch 46/120, Loss: 0.4634, Accuracy: 0.7327\n",
      "Epoch 47/120, Loss: 0.4629, Accuracy: 0.7323\n",
      "Epoch 48/120, Loss: 0.4631, Accuracy: 0.7327\n",
      "Epoch 49/120, Loss: 0.4627, Accuracy: 0.7335\n",
      "Epoch 50/120, Loss: 0.4629, Accuracy: 0.7331\n",
      "Epoch 51/120, Loss: 0.4628, Accuracy: 0.7340\n",
      "Epoch 52/120, Loss: 0.4627, Accuracy: 0.7333\n",
      "Epoch 53/120, Loss: 0.4624, Accuracy: 0.7334\n",
      "Epoch 54/120, Loss: 0.4623, Accuracy: 0.7338\n",
      "Epoch 55/120, Loss: 0.4622, Accuracy: 0.7338\n",
      "Epoch 56/120, Loss: 0.4623, Accuracy: 0.7342\n",
      "Epoch 57/120, Loss: 0.4619, Accuracy: 0.7335\n",
      "Epoch 58/120, Loss: 0.4622, Accuracy: 0.7343\n",
      "Epoch 59/120, Loss: 0.4617, Accuracy: 0.7337\n",
      "Epoch 60/120, Loss: 0.4618, Accuracy: 0.7340\n",
      "Epoch 61/120, Loss: 0.4617, Accuracy: 0.7338\n",
      "Epoch 62/120, Loss: 0.4615, Accuracy: 0.7337\n",
      "Epoch 63/120, Loss: 0.4614, Accuracy: 0.7343\n",
      "Epoch 64/120, Loss: 0.4616, Accuracy: 0.7348\n",
      "Epoch 65/120, Loss: 0.4614, Accuracy: 0.7340\n",
      "Epoch 66/120, Loss: 0.4616, Accuracy: 0.7346\n",
      "Epoch 67/120, Loss: 0.4614, Accuracy: 0.7338\n",
      "Epoch 68/120, Loss: 0.4611, Accuracy: 0.7346\n",
      "Epoch 69/120, Loss: 0.4611, Accuracy: 0.7349\n",
      "Epoch 70/120, Loss: 0.4610, Accuracy: 0.7349\n",
      "Epoch 71/120, Loss: 0.4611, Accuracy: 0.7345\n",
      "Epoch 72/120, Loss: 0.4611, Accuracy: 0.7349\n",
      "Epoch 73/120, Loss: 0.4608, Accuracy: 0.7352\n",
      "Epoch 74/120, Loss: 0.4611, Accuracy: 0.7352\n",
      "Epoch 75/120, Loss: 0.4609, Accuracy: 0.7349\n",
      "Epoch 76/120, Loss: 0.4609, Accuracy: 0.7346\n",
      "Epoch 77/120, Loss: 0.4604, Accuracy: 0.7359\n",
      "Epoch 78/120, Loss: 0.4608, Accuracy: 0.7347\n",
      "Epoch 79/120, Loss: 0.4609, Accuracy: 0.7344\n",
      "Epoch 80/120, Loss: 0.4608, Accuracy: 0.7348\n",
      "Epoch 81/120, Loss: 0.4606, Accuracy: 0.7352\n",
      "Epoch 82/120, Loss: 0.4608, Accuracy: 0.7344\n",
      "Epoch 83/120, Loss: 0.4606, Accuracy: 0.7343\n",
      "Epoch 84/120, Loss: 0.4606, Accuracy: 0.7350\n",
      "Epoch 85/120, Loss: 0.4607, Accuracy: 0.7353\n",
      "Epoch 86/120, Loss: 0.4605, Accuracy: 0.7348\n",
      "Epoch 87/120, Loss: 0.4607, Accuracy: 0.7346\n",
      "Epoch 88/120, Loss: 0.4604, Accuracy: 0.7353\n",
      "Epoch 89/120, Loss: 0.4605, Accuracy: 0.7354\n",
      "Epoch 90/120, Loss: 0.4605, Accuracy: 0.7342\n",
      "Epoch 91/120, Loss: 0.4603, Accuracy: 0.7355\n",
      "Epoch 92/120, Loss: 0.4603, Accuracy: 0.7352\n",
      "Epoch 93/120, Loss: 0.4602, Accuracy: 0.7346\n",
      "Epoch 94/120, Loss: 0.4603, Accuracy: 0.7350\n",
      "Epoch 95/120, Loss: 0.4604, Accuracy: 0.7351\n",
      "Epoch 96/120, Loss: 0.4604, Accuracy: 0.7346\n",
      "Epoch 97/120, Loss: 0.4602, Accuracy: 0.7352\n",
      "Epoch 98/120, Loss: 0.4600, Accuracy: 0.7351\n",
      "Epoch 99/120, Loss: 0.4600, Accuracy: 0.7345\n",
      "Epoch 100/120, Loss: 0.4601, Accuracy: 0.7352\n",
      "Epoch 101/120, Loss: 0.4603, Accuracy: 0.7346\n",
      "Epoch 102/120, Loss: 0.4601, Accuracy: 0.7351\n",
      "Epoch 103/120, Loss: 0.4599, Accuracy: 0.7352\n",
      "Epoch 104/120, Loss: 0.4599, Accuracy: 0.7351\n",
      "Epoch 105/120, Loss: 0.4599, Accuracy: 0.7353\n",
      "Epoch 106/120, Loss: 0.4601, Accuracy: 0.7350\n",
      "Epoch 107/120, Loss: 0.4599, Accuracy: 0.7344\n",
      "Epoch 108/120, Loss: 0.4599, Accuracy: 0.7351\n",
      "Epoch 109/120, Loss: 0.4598, Accuracy: 0.7350\n",
      "Epoch 110/120, Loss: 0.4600, Accuracy: 0.7344\n",
      "Epoch 111/120, Loss: 0.4600, Accuracy: 0.7347\n",
      "Epoch 112/120, Loss: 0.4599, Accuracy: 0.7344\n",
      "Epoch 113/120, Loss: 0.4598, Accuracy: 0.7346\n",
      "Epoch 114/120, Loss: 0.4599, Accuracy: 0.7347\n",
      "Epoch 115/120, Loss: 0.4599, Accuracy: 0.7351\n",
      "Epoch 116/120, Loss: 0.4599, Accuracy: 0.7348\n",
      "Epoch 117/120, Loss: 0.4596, Accuracy: 0.7350\n",
      "Epoch 118/120, Loss: 0.4596, Accuracy: 0.7350\n",
      "Epoch 119/120, Loss: 0.4595, Accuracy: 0.7348\n",
      "Epoch 120/120, Loss: 0.4598, Accuracy: 0.7349\n",
      "Training complete!\n",
      "Accuracy: 0.7142\n",
      "Epoch 1/120, Loss: 0.5016, Accuracy: 0.7060\n",
      "Epoch 2/120, Loss: 0.4868, Accuracy: 0.7115\n",
      "Epoch 3/120, Loss: 0.4831, Accuracy: 0.7149\n",
      "Epoch 4/120, Loss: 0.4806, Accuracy: 0.7178\n",
      "Epoch 5/120, Loss: 0.4789, Accuracy: 0.7189\n",
      "Epoch 6/120, Loss: 0.4772, Accuracy: 0.7209\n",
      "Epoch 7/120, Loss: 0.4758, Accuracy: 0.7227\n",
      "Epoch 8/120, Loss: 0.4743, Accuracy: 0.7239\n",
      "Epoch 9/120, Loss: 0.4732, Accuracy: 0.7245\n",
      "Epoch 10/120, Loss: 0.4725, Accuracy: 0.7261\n",
      "Epoch 11/120, Loss: 0.4714, Accuracy: 0.7264\n",
      "Epoch 12/120, Loss: 0.4705, Accuracy: 0.7273\n",
      "Epoch 13/120, Loss: 0.4696, Accuracy: 0.7286\n",
      "Epoch 14/120, Loss: 0.4689, Accuracy: 0.7294\n",
      "Epoch 15/120, Loss: 0.4680, Accuracy: 0.7290\n",
      "Epoch 16/120, Loss: 0.4673, Accuracy: 0.7293\n",
      "Epoch 17/120, Loss: 0.4668, Accuracy: 0.7308\n",
      "Epoch 18/120, Loss: 0.4659, Accuracy: 0.7319\n",
      "Epoch 19/120, Loss: 0.4653, Accuracy: 0.7316\n",
      "Epoch 20/120, Loss: 0.4649, Accuracy: 0.7317\n",
      "Epoch 21/120, Loss: 0.4640, Accuracy: 0.7324\n",
      "Epoch 22/120, Loss: 0.4637, Accuracy: 0.7321\n",
      "Epoch 23/120, Loss: 0.4632, Accuracy: 0.7333\n",
      "Epoch 24/120, Loss: 0.4626, Accuracy: 0.7334\n",
      "Epoch 25/120, Loss: 0.4621, Accuracy: 0.7340\n",
      "Epoch 26/120, Loss: 0.4617, Accuracy: 0.7340\n",
      "Epoch 27/120, Loss: 0.4614, Accuracy: 0.7353\n",
      "Epoch 28/120, Loss: 0.4609, Accuracy: 0.7342\n",
      "Epoch 29/120, Loss: 0.4605, Accuracy: 0.7355\n",
      "Epoch 30/120, Loss: 0.4601, Accuracy: 0.7360\n",
      "Epoch 31/120, Loss: 0.4600, Accuracy: 0.7366\n",
      "Epoch 32/120, Loss: 0.4593, Accuracy: 0.7365\n",
      "Epoch 33/120, Loss: 0.4591, Accuracy: 0.7369\n",
      "Epoch 34/120, Loss: 0.4585, Accuracy: 0.7374\n",
      "Epoch 35/120, Loss: 0.4583, Accuracy: 0.7369\n",
      "Epoch 36/120, Loss: 0.4580, Accuracy: 0.7364\n",
      "Epoch 37/120, Loss: 0.4578, Accuracy: 0.7377\n",
      "Epoch 38/120, Loss: 0.4573, Accuracy: 0.7377\n",
      "Epoch 39/120, Loss: 0.4570, Accuracy: 0.7383\n",
      "Epoch 40/120, Loss: 0.4569, Accuracy: 0.7379\n",
      "Epoch 41/120, Loss: 0.4564, Accuracy: 0.7381\n",
      "Epoch 42/120, Loss: 0.4562, Accuracy: 0.7392\n",
      "Epoch 43/120, Loss: 0.4561, Accuracy: 0.7391\n",
      "Epoch 44/120, Loss: 0.4557, Accuracy: 0.7385\n",
      "Epoch 45/120, Loss: 0.4555, Accuracy: 0.7399\n",
      "Epoch 46/120, Loss: 0.4556, Accuracy: 0.7397\n",
      "Epoch 47/120, Loss: 0.4552, Accuracy: 0.7396\n",
      "Epoch 48/120, Loss: 0.4549, Accuracy: 0.7397\n",
      "Epoch 49/120, Loss: 0.4548, Accuracy: 0.7395\n",
      "Epoch 50/120, Loss: 0.4547, Accuracy: 0.7391\n",
      "Epoch 51/120, Loss: 0.4544, Accuracy: 0.7401\n",
      "Epoch 52/120, Loss: 0.4541, Accuracy: 0.7408\n",
      "Epoch 53/120, Loss: 0.4539, Accuracy: 0.7408\n",
      "Epoch 54/120, Loss: 0.4538, Accuracy: 0.7404\n",
      "Epoch 55/120, Loss: 0.4534, Accuracy: 0.7411\n",
      "Epoch 56/120, Loss: 0.4534, Accuracy: 0.7405\n",
      "Epoch 57/120, Loss: 0.4533, Accuracy: 0.7410\n",
      "Epoch 58/120, Loss: 0.4531, Accuracy: 0.7417\n",
      "Epoch 59/120, Loss: 0.4527, Accuracy: 0.7409\n",
      "Epoch 60/120, Loss: 0.4527, Accuracy: 0.7411\n",
      "Epoch 61/120, Loss: 0.4526, Accuracy: 0.7413\n",
      "Epoch 62/120, Loss: 0.4524, Accuracy: 0.7412\n",
      "Epoch 63/120, Loss: 0.4521, Accuracy: 0.7415\n",
      "Epoch 64/120, Loss: 0.4521, Accuracy: 0.7417\n",
      "Epoch 65/120, Loss: 0.4517, Accuracy: 0.7423\n",
      "Epoch 66/120, Loss: 0.4517, Accuracy: 0.7419\n",
      "Epoch 67/120, Loss: 0.4516, Accuracy: 0.7426\n",
      "Epoch 68/120, Loss: 0.4513, Accuracy: 0.7425\n",
      "Epoch 69/120, Loss: 0.4513, Accuracy: 0.7425\n",
      "Epoch 70/120, Loss: 0.4513, Accuracy: 0.7419\n",
      "Epoch 71/120, Loss: 0.4512, Accuracy: 0.7424\n",
      "Epoch 72/120, Loss: 0.4509, Accuracy: 0.7430\n",
      "Epoch 73/120, Loss: 0.4507, Accuracy: 0.7423\n",
      "Epoch 74/120, Loss: 0.4508, Accuracy: 0.7423\n",
      "Epoch 75/120, Loss: 0.4505, Accuracy: 0.7422\n",
      "Epoch 76/120, Loss: 0.4507, Accuracy: 0.7419\n",
      "Epoch 77/120, Loss: 0.4504, Accuracy: 0.7424\n",
      "Epoch 78/120, Loss: 0.4503, Accuracy: 0.7428\n",
      "Epoch 79/120, Loss: 0.4503, Accuracy: 0.7424\n",
      "Epoch 80/120, Loss: 0.4503, Accuracy: 0.7435\n",
      "Epoch 81/120, Loss: 0.4501, Accuracy: 0.7433\n",
      "Epoch 82/120, Loss: 0.4499, Accuracy: 0.7430\n",
      "Epoch 83/120, Loss: 0.4498, Accuracy: 0.7428\n",
      "Epoch 84/120, Loss: 0.4496, Accuracy: 0.7432\n",
      "Epoch 85/120, Loss: 0.4496, Accuracy: 0.7428\n",
      "Epoch 86/120, Loss: 0.4496, Accuracy: 0.7427\n",
      "Epoch 87/120, Loss: 0.4494, Accuracy: 0.7430\n",
      "Epoch 88/120, Loss: 0.4489, Accuracy: 0.7438\n",
      "Epoch 89/120, Loss: 0.4492, Accuracy: 0.7437\n",
      "Epoch 90/120, Loss: 0.4491, Accuracy: 0.7440\n",
      "Epoch 91/120, Loss: 0.4488, Accuracy: 0.7426\n",
      "Epoch 92/120, Loss: 0.4488, Accuracy: 0.7431\n",
      "Epoch 93/120, Loss: 0.4488, Accuracy: 0.7441\n",
      "Epoch 94/120, Loss: 0.4488, Accuracy: 0.7431\n",
      "Epoch 95/120, Loss: 0.4488, Accuracy: 0.7432\n",
      "Epoch 96/120, Loss: 0.4486, Accuracy: 0.7437\n",
      "Epoch 97/120, Loss: 0.4483, Accuracy: 0.7434\n",
      "Epoch 98/120, Loss: 0.4484, Accuracy: 0.7438\n",
      "Epoch 99/120, Loss: 0.4485, Accuracy: 0.7435\n",
      "Epoch 100/120, Loss: 0.4480, Accuracy: 0.7447\n",
      "Epoch 101/120, Loss: 0.4481, Accuracy: 0.7444\n",
      "Epoch 102/120, Loss: 0.4481, Accuracy: 0.7437\n",
      "Epoch 103/120, Loss: 0.4482, Accuracy: 0.7437\n",
      "Epoch 104/120, Loss: 0.4480, Accuracy: 0.7439\n",
      "Epoch 105/120, Loss: 0.4479, Accuracy: 0.7450\n",
      "Epoch 106/120, Loss: 0.4478, Accuracy: 0.7439\n",
      "Epoch 107/120, Loss: 0.4478, Accuracy: 0.7442\n",
      "Epoch 108/120, Loss: 0.4478, Accuracy: 0.7447\n",
      "Epoch 109/120, Loss: 0.4477, Accuracy: 0.7440\n",
      "Epoch 110/120, Loss: 0.4476, Accuracy: 0.7452\n",
      "Epoch 111/120, Loss: 0.4475, Accuracy: 0.7448\n",
      "Epoch 112/120, Loss: 0.4474, Accuracy: 0.7444\n",
      "Epoch 113/120, Loss: 0.4473, Accuracy: 0.7449\n",
      "Epoch 114/120, Loss: 0.4476, Accuracy: 0.7446\n",
      "Epoch 115/120, Loss: 0.4473, Accuracy: 0.7441\n",
      "Epoch 116/120, Loss: 0.4473, Accuracy: 0.7451\n",
      "Epoch 117/120, Loss: 0.4472, Accuracy: 0.7449\n",
      "Epoch 118/120, Loss: 0.4472, Accuracy: 0.7447\n",
      "Epoch 119/120, Loss: 0.4473, Accuracy: 0.7445\n",
      "Epoch 120/120, Loss: 0.4471, Accuracy: 0.7450\n",
      "Training complete!\n",
      "Accuracy: 0.7145\n",
      "{8: 0.7097050021376656, 16: 0.7141836880445651, 32: 0.7144892434819775}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data(language_importances_larger, data, sequence_length=seq, vocab_size=3)\n",
    "    training_results = {}\n",
    "    for batch_size in [8, 16, 32]:\n",
    "        dataset = PositionDataset(larger_inputs, larger_labels, \"cpu\")\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        input_size = larger_inputs.shape[1]\n",
    "        accuracy, model = train(120, dataloader, input_size, 0.001, \"cpu\")\n",
    "\n",
    "        test_importances, test_data, test_labels = test_perturbation(env, agent, number_samples=10000)\n",
    "        test_importances_larger = np.where(test_importances > 0.02)\n",
    "        test_inputs, test_labels = generate_training_data(test_importances_larger, test_data, sequence_length=seq, vocab_size=3)\n",
    "        test_dataset = PositionDataset(test_inputs, test_labels, \"cpu\")\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "        test_accuracy = test(model, test_dataloader, \"cpu\")\n",
    "\n",
    "        training_results[batch_size] = test_accuracy\n",
    "\n",
    "    print(training_results)\n",
    "\n",
    "    results[seq][\"above threshold\"] = test_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'above threshold': 0.6827969711310933}, 2: {'above threshold': 0.7241631162507608}, 3: {'above threshold': 0.7144892434819775}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'below threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m latex_table\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generate and print the LaTeX table\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m latex_table \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_latex_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(latex_table)\n",
      "Cell \u001b[0;32mIn[70], line 8\u001b[0m, in \u001b[0;36mgenerate_latex_table\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m latex_table \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhline\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_len, values \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 8\u001b[0m     above_threshold_len \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbelow threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m     noised_share_len \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabove threshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     all_utterances \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'below threshold'"
     ]
    }
   ],
   "source": [
    "def generate_latex_table(data):\n",
    "    latex_table = \"\\\\begin{table}[h!]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|c|c|}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Seq} & \\\\textbf{All} & \\\\textbf{\\\\textless T=0.02} & \\\\textbf{\\\\textgreater T=0.02} \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    for seq_len, values in data.items():\n",
    "        above_threshold_len = values['below threshold']\n",
    "        noised_share_len = values['above threshold']\n",
    "        all_utterances = values['all']\n",
    "        latex_table += f\"{seq_len} & {all_utterances:.3f} & {above_threshold_len:.3f} & {noised_share_len:.3f} \\\\\\\\\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    latex_table += \"\\\\end{tabular}\\n\\\\caption{MA Pong Diagnostic classifier results}\\n\\\\label{table:ma_pong_classifier}\\n\\\\end{table}\\n\"\n",
    "    return latex_table\n",
    "\n",
    "# Generate and print the LaTeX table\n",
    "latex_table = generate_latex_table(results)\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
