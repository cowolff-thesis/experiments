{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ThesisPackage.Environments.multi_pong_sender_receiver_ball_onehot import PongEnvSenderReceiverBallOneHot\n",
    "from ThesisPackage.RL.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from ThesisPackage.RL.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "from ThesisPackage.Wrappers.vecWrapper import PettingZooVectorizationParallelWrapper\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(max_episode_steps = 1024, sequence_length = 1, vocab_size = 3):\n",
    "    env = PongEnvSenderReceiverBallOneHot(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_episode_steps=max_episode_steps)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", sequence_length=1, vocab_size=3):\n",
    "    env = make_env(sequence_length=sequence_length, vocab_size=vocab_size)\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pong\" in model:\n",
    "            state_dict = torch.load(os.path.join(path, model))\n",
    "            timestamp = model.split(\"_\")[-1]\n",
    "            timestamp = int(timestamp.split(\".\")[0])\n",
    "            agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "            agent.agent.load_state_dict(state_dict)\n",
    "            models[timestamp] = agent\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_perturbation(env, agent, threshold=0.02, number_samples=30000):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "    data = {\"paddle_1\": [], \"paddle_2\": [], \"paddle_1_obs\": [], \"paddle_2_obs\": [], \"ball_1_pos\": [], \"ball_2_pos\": []}\n",
    "\n",
    "    with tqdm(total=number_samples) as pbar:\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            for cur_agent in env.agents:\n",
    "                data[cur_agent].append(copy.deepcopy(env.paddles[cur_agent]))\n",
    "                data[cur_agent + \"_obs\"].append(copy.deepcopy(obs[cur_agent]))\n",
    "            for cur_ball in env.balls.keys():\n",
    "                data[cur_ball + \"_pos\"].append(copy.deepcopy(env.balls[cur_ball][\"position\"]))\n",
    "            obs = [obs]\n",
    "            state = [state]\n",
    "            obs = np.array(flatten_list(obs))\n",
    "            state = np.array(flatten_list(state))\n",
    "            \n",
    "            # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "            language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "            language_importances.append(language_perturbation)\n",
    "\n",
    "            if np.any(language_perturbation > threshold):\n",
    "                pbar.update(1)\n",
    "                timestep += 1\n",
    "\n",
    "            if timestep > number_samples:\n",
    "                break\n",
    "\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "                actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "            actions = actions[0]\n",
    "            actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "            obs, _, truncations, terminations, infos = env.step(actions)\n",
    "            state = env.state()\n",
    "\n",
    "            if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "                average_length.append(env.timestep)\n",
    "                obs, info = env.reset()\n",
    "                state = env.state()\n",
    "    return np.array(language_importances), data, average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 2: Define the dataset and dataloader\n",
    "class PositionDataset(Dataset):\n",
    "    def __init__(self, data, labels, device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Step 3: Define the model architecture\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, dataloader, input_size, learning_rate, device):\n",
    "    \n",
    "    model = SimpleClassifier(input_size).to(device)\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        # scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return epoch_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chance_level(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "\n",
    "    chance_level = counts[index] / len(labels)\n",
    "    return chance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_training_data(language_importances_indices, data, sequence_length=1, vocab_size=3):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    pos_indices = paddle_1_indices[3:-6]\n",
    "    pos_2_indices = [index + 6 for index in pos_indices]\n",
    "\n",
    "    targets = [np.array(data[\"ball_1_pos\"]), np.array(data[\"ball_2_pos\"])]\n",
    "\n",
    "    distances = [abs(np.array(data[\"paddle_1\"])[pos_2_indices] - target[pos_2_indices, 1]) for target in targets]\n",
    "\n",
    "    labels = np.argmin(distances, axis=0)\n",
    "\n",
    "    paddle_1_obs = np.array(data[\"paddle_1_obs\"])\n",
    "    paddle_1_obs = np.array([paddle_1_obs[index - 3:index + 3] for index in pos_indices])\n",
    "    player_1_lang = paddle_1_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_1_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_1_lang = player_1_lang.reshape(new_shape)\n",
    "\n",
    "    paddle_2_obs = np.array(data[\"paddle_2_obs\"])\n",
    "    paddle_2_obs = np.array([paddle_2_obs[index - 3:index + 3] for index in pos_indices])\n",
    "    player_2_lang = paddle_2_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_2_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_2_lang = player_2_lang.reshape(new_shape)\n",
    "\n",
    "    inputs = np.concatenate((player_1_lang, player_2_lang), axis=1)\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def generate_training_data_obs(language_importances_indices, data, sequence_length=1, vocab_size=3, length=20, height=20, noise=False):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "\n",
    "    labels = np.zeros_like(paddle_1_indices)\n",
    "    labels = np.where(np.array(data[\"paddle_1\"])[paddle_1_indices] > np.array(data[\"paddle_2\"])[paddle_1_indices], 1, 0)\n",
    "\n",
    "    inputs = np.array(data[\"paddle_1_obs\"])[paddle_1_indices]\n",
    "\n",
    "    if noise:\n",
    "        random_utterances = np.random.randint(low=0, high=vocab_size, size=(inputs.shape[0], sequence_length))\n",
    "        random_utterances = np.eye(vocab_size)[random_utterances]\n",
    "        random_utterances = random_utterances.reshape((inputs.shape[0], sequence_length * vocab_size))\n",
    "        inputs[:, -1 * sequence_length * vocab_size:] = random_utterances\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directories = os.listdir(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/\")\n",
    "directories = [model for model in directories if \".DS_Store\" not in model]\n",
    "models = {}\n",
    "for directory in directories:\n",
    "    sequence_length = int(directory.split(\"_\")[-1])\n",
    "    agents = load(f\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/{directory}\", sequence_length=sequence_length)\n",
    "    agent_indizes = list(agents.keys())\n",
    "    agent_indizes.sort()\n",
    "    models[sequence_length] = agents[agent_indizes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Loss: 0.6277, Accuracy: 0.6096\n",
      "Epoch 2/120, Loss: 0.6246, Accuracy: 0.6104\n",
      "Epoch 3/120, Loss: 0.6236, Accuracy: 0.6104\n",
      "Epoch 4/120, Loss: 0.6239, Accuracy: 0.6107\n",
      "Epoch 5/120, Loss: 0.6235, Accuracy: 0.6101\n",
      "Epoch 6/120, Loss: 0.6238, Accuracy: 0.6115\n",
      "Epoch 7/120, Loss: 0.6234, Accuracy: 0.6116\n",
      "Epoch 8/120, Loss: 0.6230, Accuracy: 0.6108\n",
      "Epoch 9/120, Loss: 0.6232, Accuracy: 0.6110\n",
      "Epoch 10/120, Loss: 0.6231, Accuracy: 0.6090\n",
      "Epoch 11/120, Loss: 0.6230, Accuracy: 0.6106\n",
      "Epoch 12/120, Loss: 0.6233, Accuracy: 0.6095\n",
      "Epoch 13/120, Loss: 0.6230, Accuracy: 0.6106\n",
      "Epoch 14/120, Loss: 0.6232, Accuracy: 0.6106\n",
      "Epoch 15/120, Loss: 0.6231, Accuracy: 0.6096\n",
      "Epoch 16/120, Loss: 0.6234, Accuracy: 0.6074\n",
      "Epoch 17/120, Loss: 0.6230, Accuracy: 0.6098\n",
      "Epoch 18/120, Loss: 0.6230, Accuracy: 0.6087\n",
      "Epoch 19/120, Loss: 0.6232, Accuracy: 0.6089\n",
      "Epoch 20/120, Loss: 0.6231, Accuracy: 0.6099\n",
      "Epoch 21/120, Loss: 0.6233, Accuracy: 0.6106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m input_size \u001b[38;5;241m=\u001b[39m larger_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m results[seq][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabove threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m accuracy\n\u001b[1;32m     16\u001b[0m test_importances, test_data, test_average_length \u001b[38;5;241m=\u001b[39m test_perturbation(env, agent, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m, number_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "Cell \u001b[0;32mIn[122], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, dataloader, input_size, learning_rate, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data(language_importances_larger, data, sequence_length=seq, vocab_size=3)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, device)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, device)\n",
    "    results[seq][\"above threshold\"] = accuracy\n",
    "\n",
    "    test_importances, test_data, test_average_length = test_perturbation(env, agent, threshold=0.02, number_samples=10000)\n",
    "    test_importances_larger = np.where(test_importances > 0.02)\n",
    "    test_inputs, test_labels = generate_training_data(test_importances_larger, test_data, sequence_length=seq, vocab_size=3)\n",
    "    test_dataset = PositionDataset(test_inputs, test_labels, device)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    accuracy = test(model, test_dataloader, device)\n",
    "    print(calculate_chance_level(test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'above threshold': 0.7572159512557122}, 2: {'above threshold': 0.8025763319880967}, 3: {'above threshold': 0.905081998474447}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300001it [33:38, 148.59it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Loss: 0.3880, Accuracy: 0.8211\n",
      "Epoch 2/120, Loss: 0.3186, Accuracy: 0.8601\n",
      "Epoch 3/120, Loss: 0.3056, Accuracy: 0.8653\n",
      "Epoch 4/120, Loss: 0.2986, Accuracy: 0.8693\n",
      "Epoch 5/120, Loss: 0.2929, Accuracy: 0.8711\n",
      "Epoch 6/120, Loss: 0.2904, Accuracy: 0.8721\n",
      "Epoch 7/120, Loss: 0.2873, Accuracy: 0.8738\n",
      "Epoch 8/120, Loss: 0.2853, Accuracy: 0.8745\n",
      "Epoch 9/120, Loss: 0.2836, Accuracy: 0.8754\n",
      "Epoch 10/120, Loss: 0.2823, Accuracy: 0.8760\n",
      "Epoch 11/120, Loss: 0.2807, Accuracy: 0.8768\n",
      "Epoch 12/120, Loss: 0.2801, Accuracy: 0.8774\n",
      "Epoch 13/120, Loss: 0.2794, Accuracy: 0.8784\n",
      "Epoch 14/120, Loss: 0.2783, Accuracy: 0.8781\n",
      "Epoch 15/120, Loss: 0.2773, Accuracy: 0.8792\n",
      "Epoch 16/120, Loss: 0.2768, Accuracy: 0.8786\n",
      "Epoch 17/120, Loss: 0.2762, Accuracy: 0.8792\n",
      "Epoch 18/120, Loss: 0.2754, Accuracy: 0.8799\n",
      "Epoch 19/120, Loss: 0.2750, Accuracy: 0.8799\n",
      "Epoch 20/120, Loss: 0.2752, Accuracy: 0.8795\n",
      "Epoch 21/120, Loss: 0.2742, Accuracy: 0.8792\n",
      "Epoch 22/120, Loss: 0.2736, Accuracy: 0.8804\n",
      "Epoch 23/120, Loss: 0.2724, Accuracy: 0.8811\n",
      "Epoch 24/120, Loss: 0.2727, Accuracy: 0.8809\n",
      "Epoch 25/120, Loss: 0.2728, Accuracy: 0.8806\n",
      "Epoch 26/120, Loss: 0.2713, Accuracy: 0.8815\n",
      "Epoch 27/120, Loss: 0.2713, Accuracy: 0.8815\n",
      "Epoch 28/120, Loss: 0.2715, Accuracy: 0.8809\n",
      "Epoch 29/120, Loss: 0.2703, Accuracy: 0.8821\n",
      "Epoch 30/120, Loss: 0.2710, Accuracy: 0.8814\n",
      "Epoch 31/120, Loss: 0.2703, Accuracy: 0.8818\n",
      "Epoch 32/120, Loss: 0.2700, Accuracy: 0.8824\n",
      "Epoch 33/120, Loss: 0.2698, Accuracy: 0.8827\n",
      "Epoch 34/120, Loss: 0.2698, Accuracy: 0.8826\n",
      "Epoch 35/120, Loss: 0.2694, Accuracy: 0.8829\n",
      "Epoch 36/120, Loss: 0.2688, Accuracy: 0.8832\n",
      "Epoch 37/120, Loss: 0.2691, Accuracy: 0.8831\n",
      "Epoch 38/120, Loss: 0.2686, Accuracy: 0.8828\n",
      "Epoch 39/120, Loss: 0.2672, Accuracy: 0.8838\n",
      "Epoch 40/120, Loss: 0.2681, Accuracy: 0.8836\n",
      "Epoch 41/120, Loss: 0.2676, Accuracy: 0.8830\n",
      "Epoch 42/120, Loss: 0.2679, Accuracy: 0.8831\n",
      "Epoch 43/120, Loss: 0.2680, Accuracy: 0.8834\n",
      "Epoch 44/120, Loss: 0.2684, Accuracy: 0.8834\n",
      "Epoch 45/120, Loss: 0.2668, Accuracy: 0.8838\n",
      "Epoch 46/120, Loss: 0.2665, Accuracy: 0.8836\n",
      "Epoch 47/120, Loss: 0.2670, Accuracy: 0.8836\n",
      "Epoch 48/120, Loss: 0.2675, Accuracy: 0.8837\n",
      "Epoch 49/120, Loss: 0.2656, Accuracy: 0.8848\n",
      "Epoch 50/120, Loss: 0.2661, Accuracy: 0.8842\n",
      "Epoch 51/120, Loss: 0.2663, Accuracy: 0.8834\n",
      "Epoch 52/120, Loss: 0.2669, Accuracy: 0.8838\n",
      "Epoch 53/120, Loss: 0.2659, Accuracy: 0.8842\n",
      "Epoch 54/120, Loss: 0.2655, Accuracy: 0.8845\n",
      "Epoch 55/120, Loss: 0.2661, Accuracy: 0.8844\n",
      "Epoch 56/120, Loss: 0.2660, Accuracy: 0.8843\n",
      "Epoch 57/120, Loss: 0.2656, Accuracy: 0.8846\n",
      "Epoch 58/120, Loss: 0.2648, Accuracy: 0.8848\n",
      "Epoch 59/120, Loss: 0.2645, Accuracy: 0.8851\n",
      "Epoch 60/120, Loss: 0.2655, Accuracy: 0.8842\n",
      "Epoch 61/120, Loss: 0.2651, Accuracy: 0.8849\n",
      "Epoch 62/120, Loss: 0.2644, Accuracy: 0.8850\n",
      "Epoch 63/120, Loss: 0.2655, Accuracy: 0.8846\n",
      "Epoch 64/120, Loss: 0.2647, Accuracy: 0.8853\n",
      "Epoch 65/120, Loss: 0.2654, Accuracy: 0.8845\n",
      "Epoch 66/120, Loss: 0.2646, Accuracy: 0.8845\n",
      "Epoch 67/120, Loss: 0.2644, Accuracy: 0.8851\n",
      "Epoch 68/120, Loss: 0.2656, Accuracy: 0.8848\n",
      "Epoch 69/120, Loss: 0.2641, Accuracy: 0.8851\n",
      "Epoch 70/120, Loss: 0.2635, Accuracy: 0.8853\n",
      "Epoch 71/120, Loss: 0.2640, Accuracy: 0.8850\n",
      "Epoch 72/120, Loss: 0.2645, Accuracy: 0.8848\n",
      "Epoch 73/120, Loss: 0.2649, Accuracy: 0.8844\n",
      "Epoch 74/120, Loss: 0.2639, Accuracy: 0.8847\n",
      "Epoch 75/120, Loss: 0.2641, Accuracy: 0.8850\n",
      "Epoch 76/120, Loss: 0.2638, Accuracy: 0.8852\n",
      "Epoch 77/120, Loss: 0.2630, Accuracy: 0.8858\n",
      "Epoch 78/120, Loss: 0.2633, Accuracy: 0.8860\n",
      "Epoch 79/120, Loss: 0.2638, Accuracy: 0.8860\n",
      "Epoch 80/120, Loss: 0.2636, Accuracy: 0.8854\n",
      "Epoch 81/120, Loss: 0.2645, Accuracy: 0.8849\n",
      "Epoch 82/120, Loss: 0.2637, Accuracy: 0.8854\n",
      "Epoch 83/120, Loss: 0.2639, Accuracy: 0.8855\n",
      "Epoch 84/120, Loss: 0.2634, Accuracy: 0.8859\n",
      "Epoch 85/120, Loss: 0.2629, Accuracy: 0.8862\n",
      "Epoch 86/120, Loss: 0.2635, Accuracy: 0.8854\n",
      "Epoch 87/120, Loss: 0.2636, Accuracy: 0.8857\n",
      "Epoch 88/120, Loss: 0.2623, Accuracy: 0.8861\n",
      "Epoch 89/120, Loss: 0.2622, Accuracy: 0.8864\n",
      "Epoch 90/120, Loss: 0.2626, Accuracy: 0.8859\n",
      "Epoch 91/120, Loss: 0.2628, Accuracy: 0.8854\n",
      "Epoch 92/120, Loss: 0.2630, Accuracy: 0.8853\n",
      "Epoch 93/120, Loss: 0.2627, Accuracy: 0.8858\n",
      "Epoch 94/120, Loss: 0.2636, Accuracy: 0.8851\n",
      "Epoch 95/120, Loss: 0.2617, Accuracy: 0.8868\n",
      "Epoch 96/120, Loss: 0.2626, Accuracy: 0.8849\n",
      "Epoch 97/120, Loss: 0.2633, Accuracy: 0.8854\n",
      "Epoch 98/120, Loss: 0.2624, Accuracy: 0.8861\n",
      "Epoch 99/120, Loss: 0.2626, Accuracy: 0.8858\n",
      "Epoch 100/120, Loss: 0.2626, Accuracy: 0.8860\n",
      "Epoch 101/120, Loss: 0.2628, Accuracy: 0.8853\n",
      "Epoch 102/120, Loss: 0.2621, Accuracy: 0.8860\n",
      "Epoch 103/120, Loss: 0.2623, Accuracy: 0.8860\n",
      "Epoch 104/120, Loss: 0.2619, Accuracy: 0.8862\n",
      "Epoch 105/120, Loss: 0.2626, Accuracy: 0.8859\n",
      "Epoch 106/120, Loss: 0.2616, Accuracy: 0.8867\n",
      "Epoch 107/120, Loss: 0.2626, Accuracy: 0.8857\n",
      "Epoch 108/120, Loss: 0.2621, Accuracy: 0.8859\n",
      "Epoch 109/120, Loss: 0.2621, Accuracy: 0.8860\n",
      "Epoch 110/120, Loss: 0.2618, Accuracy: 0.8866\n",
      "Epoch 111/120, Loss: 0.2622, Accuracy: 0.8860\n",
      "Epoch 112/120, Loss: 0.2614, Accuracy: 0.8871\n",
      "Epoch 113/120, Loss: 0.2607, Accuracy: 0.8869\n",
      "Epoch 114/120, Loss: 0.2618, Accuracy: 0.8866\n",
      "Epoch 115/120, Loss: 0.2615, Accuracy: 0.8861\n",
      "Epoch 116/120, Loss: 0.2614, Accuracy: 0.8861\n",
      "Epoch 117/120, Loss: 0.2615, Accuracy: 0.8870\n",
      "Epoch 118/120, Loss: 0.2614, Accuracy: 0.8868\n",
      "Epoch 119/120, Loss: 0.2621, Accuracy: 0.8862\n",
      "Epoch 120/120, Loss: 0.2617, Accuracy: 0.8862\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30001it [04:02, 123.90it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300001it [1:07:45, 73.80it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Loss: 0.2927, Accuracy: 0.8762\n",
      "Epoch 2/120, Loss: 0.2220, Accuracy: 0.9111\n",
      "Epoch 3/120, Loss: 0.2073, Accuracy: 0.9164\n",
      "Epoch 4/120, Loss: 0.2001, Accuracy: 0.9198\n",
      "Epoch 5/120, Loss: 0.1949, Accuracy: 0.9213\n",
      "Epoch 6/120, Loss: 0.1914, Accuracy: 0.9233\n",
      "Epoch 7/120, Loss: 0.1889, Accuracy: 0.9237\n",
      "Epoch 8/120, Loss: 0.1867, Accuracy: 0.9247\n",
      "Epoch 9/120, Loss: 0.1842, Accuracy: 0.9259\n",
      "Epoch 10/120, Loss: 0.1824, Accuracy: 0.9269\n",
      "Epoch 11/120, Loss: 0.1809, Accuracy: 0.9273\n",
      "Epoch 12/120, Loss: 0.1797, Accuracy: 0.9279\n",
      "Epoch 13/120, Loss: 0.1779, Accuracy: 0.9279\n",
      "Epoch 14/120, Loss: 0.1771, Accuracy: 0.9285\n",
      "Epoch 15/120, Loss: 0.1763, Accuracy: 0.9288\n",
      "Epoch 16/120, Loss: 0.1758, Accuracy: 0.9290\n",
      "Epoch 17/120, Loss: 0.1753, Accuracy: 0.9301\n",
      "Epoch 18/120, Loss: 0.1733, Accuracy: 0.9301\n",
      "Epoch 19/120, Loss: 0.1732, Accuracy: 0.9305\n",
      "Epoch 20/120, Loss: 0.1724, Accuracy: 0.9311\n",
      "Epoch 21/120, Loss: 0.1716, Accuracy: 0.9311\n",
      "Epoch 22/120, Loss: 0.1711, Accuracy: 0.9308\n",
      "Epoch 23/120, Loss: 0.1706, Accuracy: 0.9317\n",
      "Epoch 24/120, Loss: 0.1696, Accuracy: 0.9319\n",
      "Epoch 25/120, Loss: 0.1696, Accuracy: 0.9317\n",
      "Epoch 26/120, Loss: 0.1695, Accuracy: 0.9321\n",
      "Epoch 27/120, Loss: 0.1688, Accuracy: 0.9324\n",
      "Epoch 28/120, Loss: 0.1688, Accuracy: 0.9324\n",
      "Epoch 29/120, Loss: 0.1675, Accuracy: 0.9332\n",
      "Epoch 30/120, Loss: 0.1679, Accuracy: 0.9326\n",
      "Epoch 31/120, Loss: 0.1667, Accuracy: 0.9335\n",
      "Epoch 32/120, Loss: 0.1679, Accuracy: 0.9326\n",
      "Epoch 33/120, Loss: 0.1666, Accuracy: 0.9332\n",
      "Epoch 34/120, Loss: 0.1663, Accuracy: 0.9335\n",
      "Epoch 35/120, Loss: 0.1670, Accuracy: 0.9334\n",
      "Epoch 36/120, Loss: 0.1656, Accuracy: 0.9337\n",
      "Epoch 37/120, Loss: 0.1658, Accuracy: 0.9340\n",
      "Epoch 38/120, Loss: 0.1645, Accuracy: 0.9339\n",
      "Epoch 39/120, Loss: 0.1647, Accuracy: 0.9340\n",
      "Epoch 40/120, Loss: 0.1647, Accuracy: 0.9341\n",
      "Epoch 41/120, Loss: 0.1647, Accuracy: 0.9341\n",
      "Epoch 42/120, Loss: 0.1645, Accuracy: 0.9349\n",
      "Epoch 43/120, Loss: 0.1633, Accuracy: 0.9345\n",
      "Epoch 44/120, Loss: 0.1638, Accuracy: 0.9347\n",
      "Epoch 45/120, Loss: 0.1639, Accuracy: 0.9342\n",
      "Epoch 46/120, Loss: 0.1636, Accuracy: 0.9352\n",
      "Epoch 47/120, Loss: 0.1634, Accuracy: 0.9345\n",
      "Epoch 48/120, Loss: 0.1633, Accuracy: 0.9347\n",
      "Epoch 49/120, Loss: 0.1634, Accuracy: 0.9350\n",
      "Epoch 50/120, Loss: 0.1636, Accuracy: 0.9346\n",
      "Epoch 51/120, Loss: 0.1634, Accuracy: 0.9345\n",
      "Epoch 52/120, Loss: 0.1622, Accuracy: 0.9350\n",
      "Epoch 53/120, Loss: 0.1628, Accuracy: 0.9353\n",
      "Epoch 54/120, Loss: 0.1621, Accuracy: 0.9350\n",
      "Epoch 55/120, Loss: 0.1627, Accuracy: 0.9352\n",
      "Epoch 56/120, Loss: 0.1617, Accuracy: 0.9355\n",
      "Epoch 57/120, Loss: 0.1623, Accuracy: 0.9356\n",
      "Epoch 58/120, Loss: 0.1619, Accuracy: 0.9349\n",
      "Epoch 59/120, Loss: 0.1619, Accuracy: 0.9356\n",
      "Epoch 60/120, Loss: 0.1612, Accuracy: 0.9355\n",
      "Epoch 61/120, Loss: 0.1617, Accuracy: 0.9357\n",
      "Epoch 62/120, Loss: 0.1610, Accuracy: 0.9359\n",
      "Epoch 63/120, Loss: 0.1600, Accuracy: 0.9360\n",
      "Epoch 64/120, Loss: 0.1615, Accuracy: 0.9357\n",
      "Epoch 65/120, Loss: 0.1616, Accuracy: 0.9349\n",
      "Epoch 66/120, Loss: 0.1615, Accuracy: 0.9358\n",
      "Epoch 67/120, Loss: 0.1599, Accuracy: 0.9362\n",
      "Epoch 68/120, Loss: 0.1607, Accuracy: 0.9356\n",
      "Epoch 69/120, Loss: 0.1606, Accuracy: 0.9359\n",
      "Epoch 70/120, Loss: 0.1599, Accuracy: 0.9361\n",
      "Epoch 71/120, Loss: 0.1605, Accuracy: 0.9359\n",
      "Epoch 72/120, Loss: 0.1606, Accuracy: 0.9361\n",
      "Epoch 73/120, Loss: 0.1604, Accuracy: 0.9358\n",
      "Epoch 74/120, Loss: 0.1603, Accuracy: 0.9361\n",
      "Epoch 75/120, Loss: 0.1597, Accuracy: 0.9366\n",
      "Epoch 76/120, Loss: 0.1590, Accuracy: 0.9368\n",
      "Epoch 77/120, Loss: 0.1608, Accuracy: 0.9359\n",
      "Epoch 78/120, Loss: 0.1594, Accuracy: 0.9365\n",
      "Epoch 79/120, Loss: 0.1597, Accuracy: 0.9360\n",
      "Epoch 80/120, Loss: 0.1600, Accuracy: 0.9358\n",
      "Epoch 81/120, Loss: 0.1600, Accuracy: 0.9362\n",
      "Epoch 82/120, Loss: 0.1586, Accuracy: 0.9366\n",
      "Epoch 83/120, Loss: 0.1588, Accuracy: 0.9371\n",
      "Epoch 84/120, Loss: 0.1592, Accuracy: 0.9366\n",
      "Epoch 85/120, Loss: 0.1599, Accuracy: 0.9364\n",
      "Epoch 86/120, Loss: 0.1596, Accuracy: 0.9365\n",
      "Epoch 87/120, Loss: 0.1582, Accuracy: 0.9365\n",
      "Epoch 88/120, Loss: 0.1592, Accuracy: 0.9365\n",
      "Epoch 89/120, Loss: 0.1593, Accuracy: 0.9368\n",
      "Epoch 90/120, Loss: 0.1586, Accuracy: 0.9366\n",
      "Epoch 91/120, Loss: 0.1587, Accuracy: 0.9363\n",
      "Epoch 92/120, Loss: 0.1590, Accuracy: 0.9366\n",
      "Epoch 93/120, Loss: 0.1587, Accuracy: 0.9367\n",
      "Epoch 94/120, Loss: 0.1582, Accuracy: 0.9366\n",
      "Epoch 95/120, Loss: 0.1584, Accuracy: 0.9365\n",
      "Epoch 96/120, Loss: 0.1587, Accuracy: 0.9365\n",
      "Epoch 97/120, Loss: 0.1588, Accuracy: 0.9368\n",
      "Epoch 98/120, Loss: 0.1584, Accuracy: 0.9366\n",
      "Epoch 99/120, Loss: 0.1573, Accuracy: 0.9374\n",
      "Epoch 100/120, Loss: 0.1579, Accuracy: 0.9368\n",
      "Epoch 101/120, Loss: 0.1576, Accuracy: 0.9371\n",
      "Epoch 102/120, Loss: 0.1576, Accuracy: 0.9368\n",
      "Epoch 103/120, Loss: 0.1575, Accuracy: 0.9368\n",
      "Epoch 104/120, Loss: 0.1580, Accuracy: 0.9368\n",
      "Epoch 105/120, Loss: 0.1578, Accuracy: 0.9370\n",
      "Epoch 106/120, Loss: 0.1589, Accuracy: 0.9370\n",
      "Epoch 107/120, Loss: 0.1580, Accuracy: 0.9368\n",
      "Epoch 108/120, Loss: 0.1570, Accuracy: 0.9378\n",
      "Epoch 109/120, Loss: 0.1581, Accuracy: 0.9369\n",
      "Epoch 110/120, Loss: 0.1584, Accuracy: 0.9363\n",
      "Epoch 111/120, Loss: 0.1572, Accuracy: 0.9370\n",
      "Epoch 112/120, Loss: 0.1574, Accuracy: 0.9370\n",
      "Epoch 113/120, Loss: 0.1581, Accuracy: 0.9371\n",
      "Epoch 114/120, Loss: 0.1574, Accuracy: 0.9368\n",
      "Epoch 115/120, Loss: 0.1570, Accuracy: 0.9374\n",
      "Epoch 116/120, Loss: 0.1570, Accuracy: 0.9371\n",
      "Epoch 117/120, Loss: 0.1575, Accuracy: 0.9372\n",
      "Epoch 118/120, Loss: 0.1572, Accuracy: 0.9369\n",
      "Epoch 119/120, Loss: 0.1577, Accuracy: 0.9367\n",
      "Epoch 120/120, Loss: 0.1569, Accuracy: 0.9374\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30001it [06:03, 82.63it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300001it [1:25:48, 58.27it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Loss: 0.3024, Accuracy: 0.8722\n",
      "Epoch 2/120, Loss: 0.2284, Accuracy: 0.9115\n",
      "Epoch 3/120, Loss: 0.2162, Accuracy: 0.9155\n",
      "Epoch 4/120, Loss: 0.2112, Accuracy: 0.9176\n",
      "Epoch 5/120, Loss: 0.2056, Accuracy: 0.9200\n",
      "Epoch 6/120, Loss: 0.2033, Accuracy: 0.9212\n",
      "Epoch 7/120, Loss: 0.1999, Accuracy: 0.9225\n",
      "Epoch 8/120, Loss: 0.1978, Accuracy: 0.9227\n",
      "Epoch 9/120, Loss: 0.1966, Accuracy: 0.9232\n",
      "Epoch 10/120, Loss: 0.1937, Accuracy: 0.9241\n",
      "Epoch 11/120, Loss: 0.1922, Accuracy: 0.9252\n",
      "Epoch 12/120, Loss: 0.1916, Accuracy: 0.9258\n",
      "Epoch 13/120, Loss: 0.1904, Accuracy: 0.9257\n",
      "Epoch 14/120, Loss: 0.1896, Accuracy: 0.9263\n",
      "Epoch 15/120, Loss: 0.1880, Accuracy: 0.9268\n",
      "Epoch 16/120, Loss: 0.1873, Accuracy: 0.9267\n",
      "Epoch 17/120, Loss: 0.1865, Accuracy: 0.9273\n",
      "Epoch 18/120, Loss: 0.1862, Accuracy: 0.9276\n",
      "Epoch 19/120, Loss: 0.1850, Accuracy: 0.9280\n",
      "Epoch 20/120, Loss: 0.1842, Accuracy: 0.9283\n",
      "Epoch 21/120, Loss: 0.1841, Accuracy: 0.9285\n",
      "Epoch 22/120, Loss: 0.1827, Accuracy: 0.9286\n",
      "Epoch 23/120, Loss: 0.1833, Accuracy: 0.9290\n",
      "Epoch 24/120, Loss: 0.1832, Accuracy: 0.9289\n",
      "Epoch 25/120, Loss: 0.1827, Accuracy: 0.9285\n",
      "Epoch 26/120, Loss: 0.1817, Accuracy: 0.9293\n",
      "Epoch 27/120, Loss: 0.1801, Accuracy: 0.9296\n",
      "Epoch 28/120, Loss: 0.1805, Accuracy: 0.9292\n",
      "Epoch 29/120, Loss: 0.1809, Accuracy: 0.9292\n",
      "Epoch 30/120, Loss: 0.1801, Accuracy: 0.9298\n",
      "Epoch 31/120, Loss: 0.1801, Accuracy: 0.9305\n",
      "Epoch 32/120, Loss: 0.1797, Accuracy: 0.9296\n",
      "Epoch 33/120, Loss: 0.1781, Accuracy: 0.9314\n",
      "Epoch 34/120, Loss: 0.1785, Accuracy: 0.9306\n",
      "Epoch 35/120, Loss: 0.1790, Accuracy: 0.9302\n",
      "Epoch 36/120, Loss: 0.1777, Accuracy: 0.9312\n",
      "Epoch 37/120, Loss: 0.1776, Accuracy: 0.9310\n",
      "Epoch 38/120, Loss: 0.1771, Accuracy: 0.9314\n",
      "Epoch 39/120, Loss: 0.1760, Accuracy: 0.9315\n",
      "Epoch 40/120, Loss: 0.1771, Accuracy: 0.9310\n",
      "Epoch 41/120, Loss: 0.1772, Accuracy: 0.9309\n",
      "Epoch 42/120, Loss: 0.1763, Accuracy: 0.9309\n",
      "Epoch 43/120, Loss: 0.1768, Accuracy: 0.9308\n",
      "Epoch 44/120, Loss: 0.1763, Accuracy: 0.9314\n",
      "Epoch 45/120, Loss: 0.1765, Accuracy: 0.9313\n",
      "Epoch 46/120, Loss: 0.1761, Accuracy: 0.9312\n",
      "Epoch 47/120, Loss: 0.1762, Accuracy: 0.9318\n",
      "Epoch 48/120, Loss: 0.1760, Accuracy: 0.9313\n",
      "Epoch 49/120, Loss: 0.1749, Accuracy: 0.9318\n",
      "Epoch 50/120, Loss: 0.1753, Accuracy: 0.9318\n",
      "Epoch 51/120, Loss: 0.1750, Accuracy: 0.9319\n",
      "Epoch 52/120, Loss: 0.1753, Accuracy: 0.9316\n",
      "Epoch 53/120, Loss: 0.1746, Accuracy: 0.9320\n",
      "Epoch 54/120, Loss: 0.1750, Accuracy: 0.9322\n",
      "Epoch 55/120, Loss: 0.1747, Accuracy: 0.9321\n",
      "Epoch 56/120, Loss: 0.1747, Accuracy: 0.9326\n",
      "Epoch 57/120, Loss: 0.1750, Accuracy: 0.9323\n",
      "Epoch 58/120, Loss: 0.1746, Accuracy: 0.9322\n",
      "Epoch 59/120, Loss: 0.1738, Accuracy: 0.9322\n",
      "Epoch 60/120, Loss: 0.1744, Accuracy: 0.9324\n",
      "Epoch 61/120, Loss: 0.1743, Accuracy: 0.9323\n",
      "Epoch 62/120, Loss: 0.1726, Accuracy: 0.9328\n",
      "Epoch 63/120, Loss: 0.1734, Accuracy: 0.9330\n",
      "Epoch 64/120, Loss: 0.1733, Accuracy: 0.9327\n",
      "Epoch 65/120, Loss: 0.1729, Accuracy: 0.9330\n",
      "Epoch 66/120, Loss: 0.1730, Accuracy: 0.9326\n",
      "Epoch 67/120, Loss: 0.1731, Accuracy: 0.9330\n",
      "Epoch 68/120, Loss: 0.1729, Accuracy: 0.9325\n",
      "Epoch 69/120, Loss: 0.1730, Accuracy: 0.9326\n",
      "Epoch 70/120, Loss: 0.1731, Accuracy: 0.9328\n",
      "Epoch 71/120, Loss: 0.1730, Accuracy: 0.9329\n",
      "Epoch 72/120, Loss: 0.1717, Accuracy: 0.9338\n",
      "Epoch 73/120, Loss: 0.1716, Accuracy: 0.9326\n",
      "Epoch 74/120, Loss: 0.1724, Accuracy: 0.9333\n",
      "Epoch 75/120, Loss: 0.1726, Accuracy: 0.9328\n",
      "Epoch 76/120, Loss: 0.1720, Accuracy: 0.9331\n",
      "Epoch 77/120, Loss: 0.1717, Accuracy: 0.9334\n",
      "Epoch 78/120, Loss: 0.1720, Accuracy: 0.9330\n",
      "Epoch 79/120, Loss: 0.1724, Accuracy: 0.9330\n",
      "Epoch 80/120, Loss: 0.1717, Accuracy: 0.9333\n",
      "Epoch 81/120, Loss: 0.1722, Accuracy: 0.9331\n",
      "Epoch 82/120, Loss: 0.1705, Accuracy: 0.9339\n",
      "Epoch 83/120, Loss: 0.1714, Accuracy: 0.9332\n",
      "Epoch 84/120, Loss: 0.1715, Accuracy: 0.9333\n",
      "Epoch 85/120, Loss: 0.1702, Accuracy: 0.9338\n",
      "Epoch 86/120, Loss: 0.1713, Accuracy: 0.9336\n",
      "Epoch 87/120, Loss: 0.1713, Accuracy: 0.9338\n",
      "Epoch 88/120, Loss: 0.1713, Accuracy: 0.9327\n",
      "Epoch 89/120, Loss: 0.1715, Accuracy: 0.9335\n",
      "Epoch 90/120, Loss: 0.1702, Accuracy: 0.9336\n",
      "Epoch 91/120, Loss: 0.1713, Accuracy: 0.9330\n",
      "Epoch 92/120, Loss: 0.1716, Accuracy: 0.9336\n",
      "Epoch 93/120, Loss: 0.1710, Accuracy: 0.9338\n",
      "Epoch 94/120, Loss: 0.1704, Accuracy: 0.9340\n",
      "Epoch 95/120, Loss: 0.1701, Accuracy: 0.9336\n",
      "Epoch 96/120, Loss: 0.1706, Accuracy: 0.9337\n",
      "Epoch 97/120, Loss: 0.1712, Accuracy: 0.9337\n",
      "Epoch 98/120, Loss: 0.1706, Accuracy: 0.9338\n",
      "Epoch 99/120, Loss: 0.1696, Accuracy: 0.9339\n",
      "Epoch 100/120, Loss: 0.1698, Accuracy: 0.9339\n",
      "Epoch 101/120, Loss: 0.1696, Accuracy: 0.9341\n",
      "Epoch 102/120, Loss: 0.1703, Accuracy: 0.9341\n",
      "Epoch 103/120, Loss: 0.1698, Accuracy: 0.9341\n",
      "Epoch 104/120, Loss: 0.1706, Accuracy: 0.9334\n",
      "Epoch 105/120, Loss: 0.1697, Accuracy: 0.9345\n",
      "Epoch 106/120, Loss: 0.1694, Accuracy: 0.9342\n",
      "Epoch 107/120, Loss: 0.1701, Accuracy: 0.9343\n",
      "Epoch 108/120, Loss: 0.1711, Accuracy: 0.9335\n",
      "Epoch 109/120, Loss: 0.1698, Accuracy: 0.9338\n",
      "Epoch 110/120, Loss: 0.1710, Accuracy: 0.9337\n",
      "Epoch 111/120, Loss: 0.1698, Accuracy: 0.9336\n",
      "Epoch 112/120, Loss: 0.1696, Accuracy: 0.9343\n",
      "Epoch 113/120, Loss: 0.1698, Accuracy: 0.9337\n",
      "Epoch 114/120, Loss: 0.1701, Accuracy: 0.9339\n",
      "Epoch 115/120, Loss: 0.1703, Accuracy: 0.9334\n",
      "Epoch 116/120, Loss: 0.1698, Accuracy: 0.9340\n",
      "Epoch 117/120, Loss: 0.1698, Accuracy: 0.9342\n",
      "Epoch 118/120, Loss: 0.1695, Accuracy: 0.9341\n",
      "Epoch 119/120, Loss: 0.1684, Accuracy: 0.9346\n",
      "Epoch 120/120, Loss: 0.1692, Accuracy: 0.9348\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30001it [09:00, 55.50it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9308\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data_obs(language_importances_larger, data, sequence_length=seq, vocab_size=4)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, \"cpu\")\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, \"cpu\")\n",
    "\n",
    "    test_language_importances, test_data, test_average_length = test_perturbation(env, agent, number_samples=30000)\n",
    "    test_language_importances_larger = np.where(test_language_importances > 0.02)\n",
    "    test_larger_inputs, test_larger_labels = generate_training_data_obs(test_language_importances_larger, test_data, sequence_length=seq, vocab_size=4)\n",
    "    test_dataset = PositionDataset(test_larger_inputs, test_larger_labels, \"cpu\")\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    test_input_size = test_larger_inputs.shape[1]\n",
    "    test_accuracy = test(model, test_dataloader, \"cpu\")\n",
    "    results[seq][\"test\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'test': 0.8816442239546421}, 2: {'test': 0.9331573604060913}, 3: {'test': 0.9308098340387084}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
